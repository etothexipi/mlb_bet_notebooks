{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore and Model StatFox Matchup Data\n",
    "`mlb_bet_notebooks/model_statfox_matchups.ipynb`\n",
    "- Explore features\n",
    "- Convert historical moneylines to break-even probabilities\n",
    "- Model pre-computed features with RF and maybe PCA\n",
    "- Compare model predictions to historical moneylines\n",
    "    - Use break-even probabilities as alternative model and compare ROC\n",
    "- Try VIF filter\n",
    "- Try k-fold CV\n",
    "- Try grid search model complexity\n",
    "- Try to get player salary\n",
    "    - Combine with addition, subtraction from statfox blobs\n",
    "- Try fix Opening Line feature \n",
    "    - Try openline probability as feature\n",
    "\n",
    "Jonathan Sims 2020-02-24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import boto3\n",
    "import pandas as pd \n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, LeaveOneOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_fill = pd.read_csv('s3://scrapes-rawhtml-dev/statfox/20200313.statfox_features.tsv.gz', sep='\\t', index_col=0)\n",
    "df_targ = pd.read_csv('s3://scrapes-rawhtml-dev/statfox/20200313.statfox_target.tsv.gz', sep='\\t', index_col=0, header=None, squeeze=True)\n",
    "df_lateline_prob = pd.read_csv('s3://scrapes-rawhtml-dev/statfox/20200313.statfox_lateline_prob.tsv.gz', sep='\\t', index_col=0, header=None, squeeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17573,)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_targ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17573, 1127)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat_fill.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        Series\n",
       "\u001b[0;31mString form:\u001b[0m\n",
       "0\n",
       "           0        1\n",
       "           1        1\n",
       "           2        0\n",
       "           3        0\n",
       "           4        1\n",
       "           5        1\n",
       "           6        1\n",
       "           7        0\n",
       "           8       <...>     0\n",
       "           17568    1\n",
       "           17569    1\n",
       "           17570    1\n",
       "           17571    0\n",
       "           17572    1\n",
       "           Name: 1, Length: 17573, dtype: int64\n",
       "\u001b[0;31mLength:\u001b[0m      17573\n",
       "\u001b[0;31mFile:\u001b[0m        ~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/series.py\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "One-dimensional ndarray with axis labels (including time series).\n",
       "\n",
       "Labels need not be unique but must be a hashable type. The object\n",
       "supports both integer- and label-based indexing and provides a host of\n",
       "methods for performing operations involving the index. Statistical\n",
       "methods from ndarray have been overridden to automatically exclude\n",
       "missing data (currently represented as NaN).\n",
       "\n",
       "Operations between Series (+, -, /, *, **) align values based on their\n",
       "associated index values-- they need not be the same length. The result\n",
       "index will be the sorted union of the two indexes.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "data : array-like, Iterable, dict, or scalar value\n",
       "    Contains data stored in Series.\n",
       "\n",
       "    .. versionchanged :: 0.23.0\n",
       "       If data is a dict, argument order is maintained for Python 3.6\n",
       "       and later.\n",
       "\n",
       "index : array-like or Index (1d)\n",
       "    Values must be hashable and have the same length as `data`.\n",
       "    Non-unique index values are allowed. Will default to\n",
       "    RangeIndex (0, 1, 2, ..., n) if not provided. If both a dict and index\n",
       "    sequence are used, the index will override the keys found in the\n",
       "    dict.\n",
       "dtype : str, numpy.dtype, or ExtensionDtype, optional\n",
       "    dtype for the output Series. If not specified, this will be\n",
       "    inferred from `data`.\n",
       "    See the :ref:`user guide <basics.dtypes>` for more usages.\n",
       "copy : bool, default False\n",
       "    Copy input data.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_targ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['matchidx']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Make sure matchidx column exists\n",
    "\n",
    "[col for col in df_feat_fill.columns if 'match' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bullpen_BB_',\n",
       " 'Bullpen_BSV',\n",
       " 'Bullpen_ERA',\n",
       " 'Bullpen_ER_',\n",
       " 'Bullpen_HR_',\n",
       " 'Bullpen_H_A',\n",
       " 'Bullpen_H_H',\n",
       " 'Bullpen_IP_',\n",
       " 'Bullpen_L_A',\n",
       " 'Bullpen_L_H',\n",
       " 'Bullpen_R_A',\n",
       " 'Bullpen_R_H',\n",
       " 'Bullpen_SO_',\n",
       " 'Bullpen_SV_',\n",
       " 'Bullpen_WHI',\n",
       " 'Bullpen_W_A',\n",
       " 'Bullpen_W_H',\n",
       " 'HitField_Te',\n",
       " 'Overall_Opp',\n",
       " 'Overall_Tea',\n",
       " 'Bullpen_H_R',\n",
       " 'Bullpen_L_R',\n",
       " 'Bullpen_R_R',\n",
       " 'Bullpen_W_R',\n",
       " 'tchidx',\n",
       " 'Bullpen_Pct',\n",
       " '_Opening_Li',\n",
       " 'nth',\n",
       " 'ar',\n",
       " '_Latest_Tot',\n",
       " '_Opening_To']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Partial header names to looks for unknown/erroneous columns\n",
    "\n",
    "f = lambda x: x[2:13]\n",
    "colstrip = pd.Series(df_feat_fill.columns).map(f)\n",
    "[col for col in colstrip.drop_duplicates() if '_h_' not in col and '_v_' not in col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate close moneyline ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_targ_dropna = df_targ[df_lateline_prob.isna() == False]\n",
    "df_lateline_prob_dropna = df_lateline_prob[df_lateline_prob.isna() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5965750121668296"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(df_targ_dropna, df_lateline_prob_dropna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17573,)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_targ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17573, 1127)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat_fill.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5550098254183768\n",
      "0.5693366606318605\n",
      "0.5622337317085262\n",
      "0.56053618588455\n",
      "0.5488892694622499\n",
      "0.5556903219342338\n",
      "0.5647986792476228\n",
      "0.5612488607538981\n",
      "0.5405110317975341\n",
      "0.5524151045501084\n"
     ]
    }
   ],
   "source": [
    "for cnt in range(0,10):\n",
    "\n",
    "    df_feat_fill_train, df_feat_fill_test, df_targ_train, df_targ_test = train_test_split(df_feat_fill, df_targ, test_size=0.2, random_state=cnt)\n",
    "\n",
    "#     clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=cnt, \n",
    "#                                  max_depth=4, \n",
    "#                                  min_samples_split=8)\n",
    "    complexity_par = {'class_weight': 'balanced', \n",
    "                      'criterion': 'entropy', \n",
    "                      'max_depth': 2, \n",
    "                      'min_samples_split': 9, \n",
    "                      'oob_score': False}\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=50, n_jobs=-1, **complexity_par)\n",
    "    \n",
    "    df_fit = clf.fit(df_feat_fill_train, df_targ_train)\n",
    "    df_pred = df_fit.predict(df_feat_fill_test)\n",
    "    \n",
    "    print(roc_auc_score(df_targ_test, df_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF with AUC and no PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17573, 1123)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat_fill.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nsplit = round(len(df_feat_fill)*0.8)\n",
    "nsplit"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[col for col in df_feat_fill.columns if '_NaN' in col]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_feat_fill_train = df_feat_fill.iloc[:nsplit]\n",
    "df_targ_train = df_targ.iloc[:nsplit]\n",
    "df_feat_fill_test = df_feat_fill.iloc[nsplit:]\n",
    "df_targ_test = df_targ.iloc[nsplit:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave one out CV to find model complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.26 s, sys: 304 ms, total: 1.57 s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "### Define len for subset for development\n",
    "\n",
    "_dev_len = 15000\n",
    "\n",
    "\n",
    "### Define feature and target data\n",
    "\n",
    "X = df_feat_fill[:_dev_len].to_numpy()\n",
    "y = df_targ[:_dev_len].to_numpy()\n",
    "\n",
    "\n",
    "### Save number of splits for leave-one-out CV\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "splits = loo.split(X)\n",
    "\n",
    "\n",
    "### Grid of hyperparams to search\n",
    "\n",
    "n_estimators = [10, 50, 100, 200]\n",
    "max_depth_par = range(1,5)\n",
    "min_samples_split_par = [5,7,9,11,14,17,20,30]\n",
    "min_samples_leaf_par = range(1,11)\n",
    "criterion_par = ['gini', 'entropy']\n",
    "class_weight_par = [None, 'balanced', 'balanced_subsample']\n",
    "oob_score_par = [True, False]\n",
    "\n",
    "parameters = {'n_estimators':n_estimators_par, \n",
    "              'max_depth':max_depth_par, \n",
    "              'min_samples_split':min_samples_split_par, \n",
    "              'min_samples_leaf':min_samples_leaf_par, \n",
    "              'criterion':criterion_par, \n",
    "              'class_weight':class_weight_par, \n",
    "              'oob_score':oob_score_par}\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=5, n_jobs=1)\n",
    "clf = GridSearchCV(rfc, parameters, n_jobs=-1, cv=3)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': 'balanced',\n",
       " 'criterion': 'entropy',\n",
       " 'max_depth': 2,\n",
       " 'min_samples_split': 9,\n",
       " 'oob_score': False}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5562666666666667"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5828358742850351"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val = df_feat_fill[_dev_len:].to_numpy()\n",
    "y_val = df_targ[_dev_len:].to_numpy()\n",
    "\n",
    "clf_pred = clf.predict(X_val)\n",
    "roc_auc_score(y_val, clf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_fill.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt in range(1,100,10):\n",
    "#     X_train, X_test, Y_train, Y_test = train_test_split(df_feat_fill, df_targ, test_size=0.2, random_state=cnt)\n",
    "\n",
    "    ### Standarize data\n",
    "\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    np_scaled = min_max_scaler.fit_transform(df_feat_fill)\n",
    "    df_feat_fill_st = pd.DataFrame(np_scaled)\n",
    "\n",
    "    df_feat_fill_train, df_feat_fill_test, df_targ_train, df_targ_test = train_test_split(df_feat_fill_st, df_targ, test_size=0.2)\n",
    "\n",
    "    clf = AdaBoostClassifier()\n",
    "    df_fit = clf.fit(df_feat_fill_train, df_targ_train)\n",
    "    df_pred = df_fit.predict(df_feat_fill_test)\n",
    "    \n",
    "    print(roc_auc_score(df_targ_test, df_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunPCA(X,n):\n",
    "    \"\"\"Takes an input data set X and returns n principal components\n",
    "    \"\"\"\n",
    "    # Create a scaler object\n",
    "    sc = StandardScaler()\n",
    "    \n",
    "    # Fit the scaler to the features and transform\n",
    "    X_std = sc.fit_transform(X)\n",
    "\n",
    "    # Create a pca object with the 2 components as a parameter\n",
    "    pca = decomposition.PCA(n_components=n)\n",
    "\n",
    "    # Fit the PCA and transform the data\n",
    "    X_std_pca = pca.fit_transform(X_std)\n",
    "    \n",
    "    return X_std_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_fill_train = df_feat_fill.iloc[:nsplit]\n",
    "df_targ_train = df_targ.iloc[:nsplit]\n",
    "df_feat_fill_test = df_feat_fill.iloc[nsplit:]\n",
    "df_targ_test = df_targ.iloc[nsplit:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create standardized, principal components\n",
    "std_pca_train = RunPCA(df_feat_fill_train,50)\n",
    "std_pca_df = RunPCA(df_feat_fill_test,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice with PCA\n",
    "\n",
    "transform df with fit on train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune n_components param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   0.5406012163200662\n",
      "6   0.5254810861839616\n",
      "11   0.5257931631733549\n",
      "16   0.5242600849629604\n",
      "21   0.5194033868155274\n",
      "26   0.5261052401627482\n",
      "31   0.5394582343464133\n",
      "36   0.5252879885467745\n",
      "41   0.5288456662258579\n",
      "46   0.530548436299235\n",
      "51   0.512311437231565\n",
      "56   0.5231171029893075\n",
      "61   0.5362496927992135\n",
      "66   0.5289373388414922\n",
      "71   0.5312876686678604\n",
      "76   0.5225846216261553\n",
      "81   0.5338584028679876\n",
      "86   0.5248198730626845\n",
      "91   0.5148587656574877\n",
      "96   0.531898169278361\n"
     ]
    }
   ],
   "source": [
    "scores = dict()\n",
    "\n",
    "for cnt in range(1,100,5):\n",
    "\n",
    "    ### Create a pca object with the 2 components as a parameter\n",
    "    \n",
    "    pca = decomposition.PCA(n_components=50)\n",
    "\n",
    "    df_feat_fill_train, df_feat_fill_test, df_targ_train, df_targ_test = train_test_split(df_feat_fill, df_targ, test_size=0.1, random_state=1)\n",
    "\n",
    "    X1 = df_feat_fill_train\n",
    "    X2 = df_feat_fill_test\n",
    "\n",
    "    ### Create a scaler object\n",
    "    \n",
    "    sc = StandardScaler()\n",
    "\n",
    "    ### Fit the scaler to the features and transform\n",
    "    \n",
    "    X1_std = sc.fit_transform(X1)\n",
    "    X2_std = sc.fit(X1).transform(X2)\n",
    "\n",
    "    ### Fit the PCA and transform the data\n",
    "    \n",
    "    X1_std_pca = pca.fit_transform(X1_std)\n",
    "    X2_std_pca = pca.fit(X1_std).transform(X2_std)\n",
    "\n",
    "    std_pca_train = X1_std_pca\n",
    "    std_pca_df = X2_std_pca\n",
    "\n",
    "    clf = AdaBoostClassifier(n_estimators=100, random_state=1)\n",
    "    df_fit = clf.fit(std_pca_train, df_targ_train)\n",
    "    df_pred = df_fit.predict(std_pca_df)\n",
    "    score = roc_auc_score(df_targ_test, df_pred)\n",
    "    \n",
    "    ### Append score to dict\n",
    "    \n",
    "    scores[cnt] = score\n",
    "    \n",
    "    ### Print for OCD\n",
    "    print(cnt,' ',score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([clf.feature_importances_, df_feat_fill_train.columns]).transpose().sort_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([clf.feature_importances_, df_feat_fill_train.columns]).transpose.sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_fill_desc = df_feat_fill_train.describe().loc[['mean', 'std']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_fill_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in df_feat_fill_desc.columns:\n",
    "    print(df_feat_fill_desc[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dict()\n",
    "for cnt in range(4):\n",
    "    scores[cnt] = cnt*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
