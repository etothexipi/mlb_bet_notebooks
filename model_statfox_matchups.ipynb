{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore and Model StatFox Matchup Data\n",
    "`mlb_bet_notebooks/model_statfox_matchups.ipynb`\n",
    "- Explore features\n",
    "- Convert historical moneylines to break-even probabilities\n",
    "- Model pre-computed features with RF and maybe PCA\n",
    "- Compare model predictions to historical moneylines\n",
    "    - Use break-even probabilities as alternative model and compare ROC\n",
    "- Try VIF filter\n",
    "- Try k-fold CV\n",
    "- Try grid search model complexity\n",
    "- Try to get player salary\n",
    "    - Combine with addition, subtraction from statfox blobs\n",
    "- Try fix Opening Line feature \n",
    "    - Try openline probability as feature\n",
    "\n",
    "Jonathan Sims 2020-02-24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import boto3\n",
    "import pandas as pd \n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "import numpy as np\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, LeaveOneOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_fill = pd.read_csv('s3://scrapes-rawhtml-dev/statfox/20200315.statfox_features.tsv.gz', sep='\\t', index_col=0)\n",
    "df_targ = pd.read_csv('s3://scrapes-rawhtml-dev/statfox/20200315.statfox_target.tsv.gz', sep='\\t', index_col=0, header=None, squeeze=True)\n",
    "df_targ_wt = pd.read_csv('s3://scrapes-rawhtml-dev/statfox/20200315.statfox_target_weight.tsv.gz', sep='\\t', index_col=0, header=None, squeeze=True)\n",
    "df_lateline_prob = pd.read_csv('s3://scrapes-rawhtml-dev/statfox/20200315.statfox_lateline_prob.tsv.gz', sep='\\t', index_col=0, header=None, squeeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17573,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_targ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17573, 1128)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat_fill.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if df_targ_wt.sum() != 1:\n",
    "    df_targ_wt = df_targ_wt / df_targ_wt.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "0    1.769231\n",
       "1    1.606061\n",
       "2    2.050000\n",
       "3    1.833333\n",
       "4    1.769231\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_targ_wt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34455.36163974437"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_targ_wt.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['matchidx']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Make sure matchidx column exists\n",
    "\n",
    "[col for col in df_feat_fill.columns if 'match' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['named: 0.1',\n",
       " 'Bullpen_BB_',\n",
       " 'Bullpen_BSV',\n",
       " 'Bullpen_ERA',\n",
       " 'Bullpen_ER_',\n",
       " 'Bullpen_HR_',\n",
       " 'Bullpen_H_A',\n",
       " 'Bullpen_H_H',\n",
       " 'Bullpen_IP_',\n",
       " 'Bullpen_L_A',\n",
       " 'Bullpen_L_H',\n",
       " 'Bullpen_R_A',\n",
       " 'Bullpen_R_H',\n",
       " 'Bullpen_SO_',\n",
       " 'Bullpen_SV_',\n",
       " 'Bullpen_WHI',\n",
       " 'Bullpen_W_A',\n",
       " 'Bullpen_W_H',\n",
       " 'HitField_Te',\n",
       " 'Overall_Opp',\n",
       " 'Overall_Tea',\n",
       " 'Bullpen_H_R',\n",
       " 'Bullpen_L_R',\n",
       " 'Bullpen_R_R',\n",
       " 'Bullpen_W_R',\n",
       " 'tchidx',\n",
       " 'Bullpen_Pct',\n",
       " '_Opening_Li',\n",
       " 'nth',\n",
       " 'ar',\n",
       " '_Latest_Tot',\n",
       " '_Opening_To']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Partial header names to looks for unknown/erroneous columns\n",
    "\n",
    "f = lambda x: x[2:13]\n",
    "colstrip = pd.Series(df_feat_fill.columns).map(f)\n",
    "[col for col in colstrip.drop_duplicates() if '_h_' not in col and '_v_' not in col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make custom score function"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_diff = (abs(y - y_pred) + 1 ) * wt - 1\n",
    "y_diff_sum = y_diff.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_score(y, y_pred, wt):\n",
    "    y_diff = (-1 * abs(y - y_pred) + 1 ) * wt - 1\n",
    "    y_diff_sum = y_diff.sum()\n",
    "    return y_diff_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_score = make_scorer(test_score, wt=pd.Series(df_targ_wt.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_scorer(test_score, wt=0        1.769231\n",
       "1        1.606061\n",
       "2        2.050000\n",
       "3        1.833333\n",
       "4        1.769231\n",
       "5        1.606061\n",
       "6        1.769231\n",
       "7        1.714286\n",
       "8        1.833333\n",
       "9        2.200000\n",
       "10       1.800000\n",
       "11       2.350000\n",
       "12       1.571429\n",
       "13       2.050000\n",
       "14       2.250000\n",
       "15       1.714286\n",
       "16       1.606061\n",
       "17       2.500000\n",
       "18       2.250000\n",
       "19       1.769231\n",
       "20       2.200000\n",
       "21       2.250000\n",
       "22       2.550000\n",
       "23       2.100000\n",
       "24       1.909091\n",
       "25       1.540541\n",
       "26       2.500000\n",
       "27       1.833333\n",
       "28       1.625000\n",
       "29       1.769231\n",
       "           ...   \n",
       "17543    1.392157\n",
       "17544    1.571429\n",
       "17545    1.645161\n",
       "17546    1.740741\n",
       "17547    1.425532\n",
       "17548    2.200000\n",
       "17549    1.384615\n",
       "17550    1.434783\n",
       "17551    2.820000\n",
       "17552    3.050000\n",
       "17553    1.740741\n",
       "17554    1.487805\n",
       "17555    1.425532\n",
       "17556    1.769231\n",
       "17557    2.200000\n",
       "17558    1.588235\n",
       "17559    1.384615\n",
       "17560    1.425532\n",
       "17561    2.250000\n",
       "17562    2.150000\n",
       "17563    1.400000\n",
       "17564    1.909091\n",
       "17565    3.150000\n",
       "17566    1.666667\n",
       "17567    1.465116\n",
       "17568    1.689655\n",
       "17569    1.606061\n",
       "17570    1.625000\n",
       "17571    2.150000\n",
       "17572    1.606061\n",
       "Length: 17573, dtype: float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'n_estimators': [200]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(test_score, wt=0\n",
       "0        1.769231\n",
       "1        1.606061\n",
       "2        2.050000\n",
       "3        1.833333\n",
       "4        1.769231\n",
       "5        1.606061\n",
       "6        1.769231\n",
       "7        1.714286\n",
       "8        1.833333\n",
       "9        2.200000\n",
       "10       1.800000\n",
       "11       2.350000\n",
       "12       1.571429\n",
       "13       2.050000\n",
       "14       2....606061\n",
       "17570    1.625000\n",
       "17571    2.150000\n",
       "17572    1.606061\n",
       "Name: 1, Length: 17573, dtype: float64),\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prof_score = make_scorer(test_score, wt=df_targ_wt)\n",
    "grid = GridSearchCV(RandomForestClassifier(n_jobs=-1), param_grid={'n_estimators':[200]}, scoring=prof_score)\n",
    "grid.fit(df_feat_fill[:14000], df_targ[:14000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-61.184986245215356"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(df_feat_fill[14000:], df_targ[14000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate close moneyline ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_targ_dropna = df_targ[df_lateline_prob.isna() == False]\n",
    "df_lateline_prob_dropna = df_lateline_prob[df_lateline_prob.isna() == False]\n",
    "df_targ_wt_dropna = df_targ_wt[df_lateline_prob.isna() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(df_targ_dropna, df_lateline_prob_dropna, sample_weight=df_targ_wt_dropna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_targ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_fill.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_targ_wt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_targ_wt.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt in range(0,10):\n",
    "\n",
    "    df_feat_fill_train, df_feat_fill_test, df_targ_train, df_targ_test, df_targ_wt_train, df_targ_wt_test = train_test_split(df_feat_fill, df_targ, df_targ_wt, test_size=0.2, random_state=cnt)\n",
    "\n",
    "#     clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=cnt, \n",
    "#                                  max_depth=4, \n",
    "#                                  min_samples_split=8)\n",
    "    complexity_par = {'class_weight': 'balanced', \n",
    "                      'criterion': 'entropy', \n",
    "                      'max_depth': 2, \n",
    "                      'min_samples_split': 9, \n",
    "                      'oob_score': False}\n",
    "    \n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, **complexity_par)\n",
    "    \n",
    "    df_fit = clf.fit(df_feat_fill_train, df_targ_train, df_targ_wt_train)\n",
    "#     df_fit = clf.fit(df_feat_fill_train, df_targ_train)\n",
    "    df_pred = df_fit.predict(df_feat_fill_test)\n",
    "    \n",
    "#     print(roc_auc_score(df_targ_test, df_pred, sample_weight=df_targ_wt_test))\n",
    "#     print(roc_auc_score(df_targ_test, df_pred))\n",
    "    print(test_score(df_targ_test, df_pred, df_targ_wt_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_fpr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)\n",
       "from prediction scores.\n",
       "\n",
       "Note: this implementation is restricted to the binary classification task\n",
       "or multilabel classification task in label indicator format.\n",
       "\n",
       "Read more in the :ref:`User Guide <roc_metrics>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "y_true : array, shape = [n_samples] or [n_samples, n_classes]\n",
       "    True binary labels or binary label indicators.\n",
       "\n",
       "y_score : array, shape = [n_samples] or [n_samples, n_classes]\n",
       "    Target scores, can either be probability estimates of the positive\n",
       "    class, confidence values, or non-thresholded measure of decisions\n",
       "    (as returned by \"decision_function\" on some classifiers). For binary\n",
       "    y_true, y_score is supposed to be the score of the class with greater\n",
       "    label.\n",
       "\n",
       "average : string, [None, 'micro', 'macro' (default), 'samples', 'weighted']\n",
       "    If ``None``, the scores for each class are returned. Otherwise,\n",
       "    this determines the type of averaging performed on the data:\n",
       "\n",
       "    ``'micro'``:\n",
       "        Calculate metrics globally by considering each element of the label\n",
       "        indicator matrix as a label.\n",
       "    ``'macro'``:\n",
       "        Calculate metrics for each label, and find their unweighted\n",
       "        mean.  This does not take label imbalance into account.\n",
       "    ``'weighted'``:\n",
       "        Calculate metrics for each label, and find their average, weighted\n",
       "        by support (the number of true instances for each label).\n",
       "    ``'samples'``:\n",
       "        Calculate metrics for each instance, and find their average.\n",
       "\n",
       "    Will be ignored when ``y_true`` is binary.\n",
       "\n",
       "sample_weight : array-like of shape = [n_samples], optional\n",
       "    Sample weights.\n",
       "\n",
       "max_fpr : float > 0 and <= 1, optional\n",
       "    If not ``None``, the standardized partial AUC [3]_ over the range\n",
       "    [0, max_fpr] is returned.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "auc : float\n",
       "\n",
       "References\n",
       "----------\n",
       ".. [1] `Wikipedia entry for the Receiver operating characteristic\n",
       "        <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n",
       "\n",
       ".. [2] Fawcett T. An introduction to ROC analysis[J]. Pattern Recognition\n",
       "       Letters, 2006, 27(8):861-874.\n",
       "\n",
       ".. [3] `Analyzing a portion of the ROC curve. McClish, 1989\n",
       "        <https://www.ncbi.nlm.nih.gov/pubmed/2668680>`_\n",
       "\n",
       "See also\n",
       "--------\n",
       "average_precision_score : Area under the precision-recall curve\n",
       "\n",
       "roc_curve : Compute Receiver operating characteristic (ROC) curve\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> from sklearn.metrics import roc_auc_score\n",
       ">>> y_true = np.array([0, 0, 1, 1])\n",
       ">>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
       ">>> roc_auc_score(y_true, y_scores)\n",
       "0.75\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/metrics/ranking.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roc_auc_score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF with AUC and no PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17573, 1123)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat_fill.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nsplit = round(len(df_feat_fill)*0.8)\n",
    "nsplit"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[col for col in df_feat_fill.columns if '_NaN' in col]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_feat_fill_train = df_feat_fill.iloc[:nsplit]\n",
    "df_targ_train = df_targ.iloc[:nsplit]\n",
    "df_feat_fill_test = df_feat_fill.iloc[nsplit:]\n",
    "df_targ_test = df_targ.iloc[nsplit:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave one out CV to find model complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "### Define len for subset for development\n",
    "\n",
    "_dev_len = 14000\n",
    "\n",
    "\n",
    "### Define feature and target data\n",
    "\n",
    "X = df_feat_fill[:_dev_len].to_numpy()\n",
    "y = df_targ[:_dev_len].to_numpy()\n",
    "\n",
    "\n",
    "### Save number of splits for leave-one-out CV\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "splits = loo.split(X)\n",
    "\n",
    "\n",
    "### Grid of hyperparams to search\n",
    "\n",
    "# n_estimators_par = [10, 50, 100, 200]\n",
    "max_depth_par = range(1,2)\n",
    "# min_samples_split_par = [5,7,9,11,14,17,20,30]\n",
    "# min_samples_leaf_par = range(1,11)\n",
    "criterion_par = ['gini', 'entropy']\n",
    "class_weight_par = [None, 'balanced', 'balanced_subsample']\n",
    "# oob_score_par = [True, False]\n",
    "\n",
    "parameters = {\n",
    "#               'n_estimators':n_estimators_par, \n",
    "              'max_depth':max_depth_par, \n",
    "#               'min_samples_split':min_samples_split_par, \n",
    "#               'min_samples_leaf':min_samples_leaf_par, \n",
    "              'criterion':criterion_par, \n",
    "              'class_weight':class_weight_par, \n",
    "#               'oob_score':oob_score_par}\n",
    "                }\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=5, n_jobs=1)\n",
    "clf = GridSearchCV(rfc, parameters, n_jobs=-1, scoring=prof_score, cv=splits)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': 'balanced',\n",
       " 'criterion': 'entropy',\n",
       " 'max_depth': 2,\n",
       " 'min_samples_split': 9,\n",
       " 'oob_score': False}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5562666666666667"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5828358742850351"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val = df_feat_fill[_dev_len:].to_numpy()\n",
    "y_val = df_targ[_dev_len:].to_numpy()\n",
    "\n",
    "clf_pred = clf.predict(X_val)\n",
    "roc_auc_score(y_val, clf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_fill.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt in range(1,100,10):\n",
    "#     X_train, X_test, Y_train, Y_test = train_test_split(df_feat_fill, df_targ, test_size=0.2, random_state=cnt)\n",
    "\n",
    "    ### Standarize data\n",
    "\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    np_scaled = min_max_scaler.fit_transform(df_feat_fill)\n",
    "    df_feat_fill_st = pd.DataFrame(np_scaled)\n",
    "\n",
    "    df_feat_fill_train, df_feat_fill_test, df_targ_train, df_targ_test = train_test_split(df_feat_fill_st, df_targ, test_size=0.2)\n",
    "\n",
    "    clf = AdaBoostClassifier()\n",
    "    df_fit = clf.fit(df_feat_fill_train, df_targ_train)\n",
    "    df_pred = df_fit.predict(df_feat_fill_test)\n",
    "    \n",
    "    print(roc_auc_score(df_targ_test, df_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunPCA(X,n):\n",
    "    \"\"\"Takes an input data set X and returns n principal components\n",
    "    \"\"\"\n",
    "    # Create a scaler object\n",
    "    sc = StandardScaler()\n",
    "    \n",
    "    # Fit the scaler to the features and transform\n",
    "    X_std = sc.fit_transform(X)\n",
    "\n",
    "    # Create a pca object with the 2 components as a parameter\n",
    "    pca = decomposition.PCA(n_components=n)\n",
    "\n",
    "    # Fit the PCA and transform the data\n",
    "    X_std_pca = pca.fit_transform(X_std)\n",
    "    \n",
    "    return X_std_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_fill_train = df_feat_fill.iloc[:nsplit]\n",
    "df_targ_train = df_targ.iloc[:nsplit]\n",
    "df_feat_fill_test = df_feat_fill.iloc[nsplit:]\n",
    "df_targ_test = df_targ.iloc[nsplit:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create standardized, principal components\n",
    "std_pca_train = RunPCA(df_feat_fill_train,50)\n",
    "std_pca_df = RunPCA(df_feat_fill_test,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice with PCA\n",
    "\n",
    "transform df with fit on train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune n_components param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   0.5406012163200662\n",
      "6   0.5254810861839616\n",
      "11   0.5257931631733549\n",
      "16   0.5242600849629604\n",
      "21   0.5194033868155274\n",
      "26   0.5261052401627482\n",
      "31   0.5394582343464133\n",
      "36   0.5252879885467745\n",
      "41   0.5288456662258579\n",
      "46   0.530548436299235\n",
      "51   0.512311437231565\n",
      "56   0.5231171029893075\n",
      "61   0.5362496927992135\n",
      "66   0.5289373388414922\n",
      "71   0.5312876686678604\n",
      "76   0.5225846216261553\n",
      "81   0.5338584028679876\n",
      "86   0.5248198730626845\n",
      "91   0.5148587656574877\n",
      "96   0.531898169278361\n"
     ]
    }
   ],
   "source": [
    "scores = dict()\n",
    "\n",
    "for cnt in range(1,100,5):\n",
    "\n",
    "    ### Create a pca object with the 2 components as a parameter\n",
    "    \n",
    "    pca = decomposition.PCA(n_components=50)\n",
    "\n",
    "    df_feat_fill_train, df_feat_fill_test, df_targ_train, df_targ_test = train_test_split(df_feat_fill, df_targ, test_size=0.1, random_state=1)\n",
    "\n",
    "    X1 = df_feat_fill_train\n",
    "    X2 = df_feat_fill_test\n",
    "\n",
    "    ### Create a scaler object\n",
    "    \n",
    "    sc = StandardScaler()\n",
    "\n",
    "    ### Fit the scaler to the features and transform\n",
    "    \n",
    "    X1_std = sc.fit_transform(X1)\n",
    "    X2_std = sc.fit(X1).transform(X2)\n",
    "\n",
    "    ### Fit the PCA and transform the data\n",
    "    \n",
    "    X1_std_pca = pca.fit_transform(X1_std)\n",
    "    X2_std_pca = pca.fit(X1_std).transform(X2_std)\n",
    "\n",
    "    std_pca_train = X1_std_pca\n",
    "    std_pca_df = X2_std_pca\n",
    "\n",
    "    clf = AdaBoostClassifier(n_estimators=100, random_state=1)\n",
    "    df_fit = clf.fit(std_pca_train, df_targ_train)\n",
    "    df_pred = df_fit.predict(std_pca_df)\n",
    "    score = roc_auc_score(df_targ_test, df_pred)\n",
    "    \n",
    "    ### Append score to dict\n",
    "    \n",
    "    scores[cnt] = score\n",
    "    \n",
    "    ### Print for OCD\n",
    "    print(cnt,' ',score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([clf.feature_importances_, df_feat_fill_train.columns]).transpose().sort_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([clf.feature_importances_, df_feat_fill_train.columns]).transpose.sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_fill_desc = df_feat_fill_train.describe().loc[['mean', 'std']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_fill_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in df_feat_fill_desc.columns:\n",
    "    print(df_feat_fill_desc[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dict()\n",
    "for cnt in range(4):\n",
    "    scores[cnt] = cnt*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
