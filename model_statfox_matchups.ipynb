{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore and Model StatFox Matchup Data\n",
    "`mlb_bet_notebooks/model_statfox_matchups.ipynb`\n",
    "- Explore features\n",
    "- Convert historical moneylines to break-even probabilities\n",
    "- Model pre-computed features with RF and maybe PCA\n",
    "- Compare model predictions to historical moneylines\n",
    "    - Use break-even probabilities as alternative model and compare ROC\n",
    "- Try VIF filter\n",
    "- Try k-fold CV\n",
    "- Try grid search model complexity\n",
    "- Try to get player salary\n",
    "    - Combine with addition, subtraction from statfox blobs\n",
    "- Try fix Opening Line feature \n",
    "    - Try openline probability as feature\n",
    "\n",
    "Jonathan Sims 2020-02-24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import boto3\n",
    "import pandas as pd \n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "import numpy as np\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, LeaveOneOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_fill = pd.read_csv('s3://scrapes-rawhtml-dev/statfox/20200315.statfox_features.tsv.gz', sep='\\t', index_col=0)\n",
    "df_targ = pd.read_csv('s3://scrapes-rawhtml-dev/statfox/20200315.statfox_target.tsv.gz', sep='\\t', index_col=0, header=None, squeeze=True)\n",
    "df_targ_wt = pd.read_csv('s3://scrapes-rawhtml-dev/statfox/20200315.statfox_target_weight.tsv.gz', sep='\\t', index_col=0, header=None, squeeze=True)\n",
    "df_lateline_prob = pd.read_csv('s3://scrapes-rawhtml-dev/statfox/20200315.statfox_lateline_prob.tsv.gz', sep='\\t', index_col=0, header=None, squeeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17573,)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_targ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17573, 1128)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat_fill.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if df_targ_wt.sum() != 1:\n",
    "    df_targ_wt = df_targ_wt / df_targ_wt.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "0    1.769231\n",
       "1    1.606061\n",
       "2    2.050000\n",
       "3    1.833333\n",
       "4    1.769231\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_targ_wt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34455.36163974437"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_targ_wt.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['matchidx']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Make sure matchidx column exists\n",
    "\n",
    "[col for col in df_feat_fill.columns if 'match' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['named: 0.1',\n",
       " 'Bullpen_BB_',\n",
       " 'Bullpen_BSV',\n",
       " 'Bullpen_ERA',\n",
       " 'Bullpen_ER_',\n",
       " 'Bullpen_HR_',\n",
       " 'Bullpen_H_A',\n",
       " 'Bullpen_H_H',\n",
       " 'Bullpen_IP_',\n",
       " 'Bullpen_L_A',\n",
       " 'Bullpen_L_H',\n",
       " 'Bullpen_R_A',\n",
       " 'Bullpen_R_H',\n",
       " 'Bullpen_SO_',\n",
       " 'Bullpen_SV_',\n",
       " 'Bullpen_WHI',\n",
       " 'Bullpen_W_A',\n",
       " 'Bullpen_W_H',\n",
       " 'HitField_Te',\n",
       " 'Overall_Opp',\n",
       " 'Overall_Tea',\n",
       " 'Bullpen_H_R',\n",
       " 'Bullpen_L_R',\n",
       " 'Bullpen_R_R',\n",
       " 'Bullpen_W_R',\n",
       " 'tchidx',\n",
       " 'Bullpen_Pct',\n",
       " '_Opening_Li',\n",
       " 'nth',\n",
       " 'ar',\n",
       " '_Latest_Tot',\n",
       " '_Opening_To']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Partial header names to looks for unknown/erroneous columns\n",
    "\n",
    "f = lambda x: x[2:13]\n",
    "colstrip = pd.Series(df_feat_fill.columns).map(f)\n",
    "[col for col in colstrip.drop_duplicates() if '_h_' not in col and '_v_' not in col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make custom score function"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_diff = (abs(y - y_pred) + 1 ) * wt - 1\n",
    "y_diff_sum = y_diff.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_score(y, y_pred, wt):\n",
    "    y_diff = (-1 * abs(y - y_pred) + 1 ) * wt - 1\n",
    "    y_diff_sum = y_diff.sum()\n",
    "    return y_diff_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_score = make_scorer(test_score, wt=pd.Series(df_targ_wt.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_scorer(test_score, wt=0        1.769231\n",
       "1        1.606061\n",
       "2        2.050000\n",
       "3        1.833333\n",
       "4        1.769231\n",
       "5        1.606061\n",
       "6        1.769231\n",
       "7        1.714286\n",
       "8        1.833333\n",
       "9        2.200000\n",
       "10       1.800000\n",
       "11       2.350000\n",
       "12       1.571429\n",
       "13       2.050000\n",
       "14       2.250000\n",
       "15       1.714286\n",
       "16       1.606061\n",
       "17       2.500000\n",
       "18       2.250000\n",
       "19       1.769231\n",
       "20       2.200000\n",
       "21       2.250000\n",
       "22       2.550000\n",
       "23       2.100000\n",
       "24       1.909091\n",
       "25       1.540541\n",
       "26       2.500000\n",
       "27       1.833333\n",
       "28       1.625000\n",
       "29       1.769231\n",
       "           ...   \n",
       "17543    1.392157\n",
       "17544    1.571429\n",
       "17545    1.645161\n",
       "17546    1.740741\n",
       "17547    1.425532\n",
       "17548    2.200000\n",
       "17549    1.384615\n",
       "17550    1.434783\n",
       "17551    2.820000\n",
       "17552    3.050000\n",
       "17553    1.740741\n",
       "17554    1.487805\n",
       "17555    1.425532\n",
       "17556    1.769231\n",
       "17557    2.200000\n",
       "17558    1.588235\n",
       "17559    1.384615\n",
       "17560    1.425532\n",
       "17561    2.250000\n",
       "17562    2.150000\n",
       "17563    1.400000\n",
       "17564    1.909091\n",
       "17565    3.150000\n",
       "17566    1.666667\n",
       "17567    1.465116\n",
       "17568    1.689655\n",
       "17569    1.606061\n",
       "17570    1.625000\n",
       "17571    2.150000\n",
       "17572    1.606061\n",
       "Length: 17573, dtype: float64)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'n_estimators': [200]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(test_score, wt=0\n",
       "0        1.769231\n",
       "1        1.606061\n",
       "2        2.050000\n",
       "3        1.833333\n",
       "4        1.769231\n",
       "5        1.606061\n",
       "6        1.769231\n",
       "7        1.714286\n",
       "8        1.833333\n",
       "9        2.200000\n",
       "10       1.800000\n",
       "11       2.350000\n",
       "12       1.571429\n",
       "13       2.050000\n",
       "14       2....606061\n",
       "17570    1.625000\n",
       "17571    2.150000\n",
       "17572    1.606061\n",
       "Name: 1, Length: 17573, dtype: float64),\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prof_score = make_scorer(test_score, wt=df_targ_wt)\n",
    "grid = GridSearchCV(RandomForestClassifier(n_jobs=-1), param_grid={'n_estimators':[200]}, scoring=prof_score)\n",
    "grid.fit(df_feat_fill[:14000], df_targ[:14000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-64.84188115895907"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(df_feat_fill[14000:], df_targ[14000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate close moneyline ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_targ_dropna = df_targ[df_lateline_prob.isna() == False]\n",
    "df_lateline_prob_dropna = df_lateline_prob[df_lateline_prob.isna() == False]\n",
    "df_targ_wt_dropna = df_targ_wt[df_lateline_prob.isna() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5071739747513021"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(df_targ_dropna, df_lateline_prob_dropna, sample_weight=df_targ_wt_dropna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17573,)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_targ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17573, 1128)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat_fill.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17573,)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_targ_wt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.76923077, 1.60606061, 2.05      , ..., 1.625     , 2.15      ,\n",
       "       1.60606061])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_targ_wt.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-36.73191414133516\n",
      "-123.22825353459302\n",
      "-111.70599247907313\n",
      "-98.32246429475701\n",
      "-145.50116764686874\n",
      "-35.54201371963056\n",
      "-119.70389741339244\n",
      "-33.07727221917111\n",
      "65.06345970393656\n",
      "-220.72166608882543\n"
     ]
    }
   ],
   "source": [
    "for cnt in range(0,10):\n",
    "\n",
    "    df_feat_fill_train, df_feat_fill_test, df_targ_train, df_targ_test, df_targ_wt_train, df_targ_wt_test = train_test_split(df_feat_fill, df_targ, df_targ_wt, test_size=0.2, random_state=cnt)\n",
    "\n",
    "#     clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=cnt, \n",
    "#                                  max_depth=4, \n",
    "#                                  min_samples_split=8)\n",
    "    complexity_par = {'class_weight': 'balanced', \n",
    "                      'criterion': 'entropy', \n",
    "                      'max_depth': 2, \n",
    "                      'min_samples_split': 9, \n",
    "                      'oob_score': False}\n",
    "    \n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, **complexity_par)\n",
    "    \n",
    "    df_fit = clf.fit(df_feat_fill_train, df_targ_train, df_targ_wt_train)\n",
    "#     df_fit = clf.fit(df_feat_fill_train, df_targ_train)\n",
    "    df_pred = df_fit.predict(df_feat_fill_test)\n",
    "    \n",
    "#     print(roc_auc_score(df_targ_test, df_pred, sample_weight=df_targ_wt_test))\n",
    "#     print(roc_auc_score(df_targ_test, df_pred))\n",
    "    print(test_score(df_targ_test, df_pred, df_targ_wt_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF with AUC and no PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17573, 1128)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat_fill.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nsplit = round(len(df_feat_fill)*0.8)\n",
    "nsplit"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[col for col in df_feat_fill.columns if '_NaN' in col]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_feat_fill_train = df_feat_fill.iloc[:nsplit]\n",
    "df_targ_train = df_targ.iloc[:nsplit]\n",
    "df_feat_fill_test = df_feat_fill.iloc[nsplit:]\n",
    "df_targ_test = df_targ.iloc[nsplit:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big gridsearch with n_jobs=1 and no sample weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "### Define len for subset for development\n",
    "\n",
    "_dev_len = 16000\n",
    "\n",
    "\n",
    "### Define feature and target data\n",
    "\n",
    "# X = df_feat_fill[:_dev_len].to_numpy()\n",
    "# y = df_targ[:_dev_len].to_numpy()\n",
    "X = df_feat_fill[:_dev_len]\n",
    "y = df_targ[:_dev_len]\n",
    "\n",
    "\n",
    "### Define custom profit score function\n",
    "\n",
    "prof_score = make_scorer(test_score, wt=df_targ_wt)\n",
    "\n",
    "\n",
    "### Save number of splits for leave-one-out CV\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "splits = loo.split(X)\n",
    "\n",
    "\n",
    "### Grid of hyperparams to search\n",
    "\n",
    "max_depth_par = range(1,30)\n",
    "min_samples_split_par = range(2,30)\n",
    "min_samples_leaf_par = range(1,30)\n",
    "criterion_par = ['gini', 'entropy']\n",
    "class_weight_par = ['balanced', 'balanced_subsample']\n",
    "oob_score_par = [False, True]\n",
    "\n",
    "parameters = {'max_depth':max_depth_par, \n",
    "              'min_samples_split':min_samples_split_par, \n",
    "              'min_samples_leaf':min_samples_leaf_par, \n",
    "              'criterion':criterion_par, \n",
    "              'class_weight':class_weight_par, \n",
    "              'oob_score':oob_score_par}\n",
    "#                 }\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100, n_jobs=1)\n",
    "clf = GridSearchCV(rfc, parameters, n_jobs=-1, scoring=prof_score, cv=10)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(df_feat_fill[_dev_len:], df_targ[_dev_len:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big gridsearch with n_jobs=2 and sample weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "### Define len for subset for development\n",
    "\n",
    "_dev_len = 16000\n",
    "\n",
    "\n",
    "### Define feature and target data\n",
    "\n",
    "# X = df_feat_fill[:_dev_len].to_numpy()\n",
    "# y = df_targ[:_dev_len].to_numpy()\n",
    "X = df_feat_fill[:_dev_len]\n",
    "y = df_targ[:_dev_len]\n",
    "\n",
    "\n",
    "### Define custom profit score function\n",
    "\n",
    "prof_score = make_scorer(test_score, wt=df_targ_wt)\n",
    "\n",
    "\n",
    "### Save number of splits for leave-one-out CV\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "splits = loo.split(X)\n",
    "\n",
    "\n",
    "### Grid of hyperparams to search\n",
    "\n",
    "max_depth_par = range(1,30)\n",
    "min_samples_split_par = range(2,30)\n",
    "min_samples_leaf_par = range(1,30)\n",
    "criterion_par = ['gini', 'entropy']\n",
    "class_weight_par = ['balanced', 'balanced_subsample']\n",
    "oob_score_par = [False, True]\n",
    "\n",
    "parameters = {'max_depth':max_depth_par, \n",
    "              'min_samples_split':min_samples_split_par, \n",
    "              'min_samples_leaf':min_samples_leaf_par, \n",
    "              'criterion':criterion_par, \n",
    "              'class_weight':class_weight_par, \n",
    "              'oob_score':oob_score_par}\n",
    "#                 }\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100, n_jobs=2)\n",
    "clf = GridSearchCV(rfc, parameters, n_jobs=-1, scoring=prof_score, cv=100)\n",
    "\n",
    "\n",
    "### Write estimator and results to pickles\n",
    "\n",
    "fit = clf.fit(X, y, sample_weight=df_targ_wt[:_dev_len])\n",
    "par = clf.best_params_\n",
    "scr = clf.score(X, y)\n",
    "sco = clf.score(df_feat_fill[_dev_len:], df_targ[_dev_len:])\n",
    "scw = clf.score(df_feat_fill[_dev_len:], df_targ[_dev_len:], sample_weight=df_targ_wt[_dev_len:])\n",
    "\n",
    "pkl = [fit, par, scr, sco, scw]\n",
    "with open(YMD+'GridSearchCV.RandomForest.pkl', 'wb') as wb:\n",
    "    pickle.dump(pkl, wb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(df_feat_fill[_dev_len:], df_targ[_dev_len:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `20200318`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.546827326160997"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val = df_feat_fill[_dev_len:].to_numpy()\n",
    "y_val = df_targ[_dev_len:].to_numpy()\n",
    "\n",
    "clf_pred = clf.predict(X_val)\n",
    "roc_auc_score(y_val, clf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_fill.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt in range(1,100,10):\n",
    "#     X_train, X_test, Y_train, Y_test = train_test_split(df_feat_fill, df_targ, test_size=0.2, random_state=cnt)\n",
    "\n",
    "    ### Standarize data\n",
    "\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    np_scaled = min_max_scaler.fit_transform(df_feat_fill)\n",
    "    df_feat_fill_st = pd.DataFrame(np_scaled)\n",
    "\n",
    "    df_feat_fill_train, df_feat_fill_test, df_targ_train, df_targ_test = train_test_split(df_feat_fill_st, df_targ, test_size=0.2)\n",
    "\n",
    "    clf = AdaBoostClassifier()\n",
    "    df_fit = clf.fit(df_feat_fill_train, df_targ_train)\n",
    "    df_pred = df_fit.predict(df_feat_fill_test)\n",
    "    \n",
    "    print(roc_auc_score(df_targ_test, df_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test out PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunPCA(X,n):\n",
    "    \"\"\"Takes an input data set X and returns n principal components\n",
    "    \"\"\"\n",
    "    # Create a scaler object\n",
    "    sc = StandardScaler()\n",
    "    \n",
    "    # Fit the scaler to the features and transform\n",
    "    X_std = sc.fit_transform(X)\n",
    "\n",
    "    # Create a pca object with the 2 components as a parameter\n",
    "    pca = decomposition.PCA(n_components=n)\n",
    "\n",
    "    # Fit the PCA and transform the data\n",
    "    X_std_pca = pca.fit_transform(X_std)\n",
    "    \n",
    "    return X_std_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_fill_train = df_feat_fill.iloc[:nsplit]\n",
    "df_targ_train = df_targ.iloc[:nsplit]\n",
    "df_feat_fill_test = df_feat_fill.iloc[nsplit:]\n",
    "df_targ_test = df_targ.iloc[nsplit:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create standardized, principal components\n",
    "std_pca_train = RunPCA(df_feat_fill_train,50)\n",
    "std_pca_df = RunPCA(df_feat_fill_test,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice with PCA\n",
    "\n",
    "transform df with fit on train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune n_components param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   0.5406012163200662\n",
      "6   0.5254810861839616\n",
      "11   0.5257931631733549\n",
      "16   0.5242600849629604\n",
      "21   0.5194033868155274\n",
      "26   0.5261052401627482\n",
      "31   0.5394582343464133\n",
      "36   0.5252879885467745\n",
      "41   0.5288456662258579\n",
      "46   0.530548436299235\n",
      "51   0.512311437231565\n",
      "56   0.5231171029893075\n",
      "61   0.5362496927992135\n",
      "66   0.5289373388414922\n",
      "71   0.5312876686678604\n",
      "76   0.5225846216261553\n",
      "81   0.5338584028679876\n",
      "86   0.5248198730626845\n",
      "91   0.5148587656574877\n",
      "96   0.531898169278361\n"
     ]
    }
   ],
   "source": [
    "scores = dict()\n",
    "\n",
    "for cnt in range(1,100,5):\n",
    "\n",
    "    ### Create a pca object with the 2 components as a parameter\n",
    "    \n",
    "    pca = decomposition.PCA(n_components=50)\n",
    "\n",
    "    df_feat_fill_train, df_feat_fill_test, df_targ_train, df_targ_test = train_test_split(df_feat_fill, df_targ, test_size=0.1, random_state=1)\n",
    "\n",
    "    X1 = df_feat_fill_train\n",
    "    X2 = df_feat_fill_test\n",
    "\n",
    "    ### Create a scaler object\n",
    "    \n",
    "    sc = StandardScaler()\n",
    "\n",
    "    ### Fit the scaler to the features and transform\n",
    "    \n",
    "    X1_std = sc.fit_transform(X1)\n",
    "    X2_std = sc.fit(X1).transform(X2)\n",
    "\n",
    "    ### Fit the PCA and transform the data\n",
    "    \n",
    "    X1_std_pca = pca.fit_transform(X1_std)\n",
    "    X2_std_pca = pca.fit(X1_std).transform(X2_std)\n",
    "\n",
    "    std_pca_train = X1_std_pca\n",
    "    std_pca_df = X2_std_pca\n",
    "\n",
    "    clf = AdaBoostClassifier(n_estimators=100, random_state=1)\n",
    "    df_fit = clf.fit(std_pca_train, df_targ_train)\n",
    "    df_pred = df_fit.predict(std_pca_df)\n",
    "    score = roc_auc_score(df_targ_test, df_pred)\n",
    "    \n",
    "    ### Append score to dict\n",
    "    \n",
    "    scores[cnt] = score\n",
    "    \n",
    "    ### Print for OCD\n",
    "    print(cnt,' ',score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([clf.feature_importances_, df_feat_fill_train.columns]).transpose().sort_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([clf.feature_importances_, df_feat_fill_train.columns]).transpose.sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_fill_desc = df_feat_fill_train.describe().loc[['mean', 'std']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_fill_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in df_feat_fill_desc.columns:\n",
    "    print(df_feat_fill_desc[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dict()\n",
    "for cnt in range(4):\n",
    "    scores[cnt] = cnt*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
