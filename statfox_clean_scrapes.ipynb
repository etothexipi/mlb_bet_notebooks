{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean StatFox Matchup Scrapes\n",
    "#### `Jonathan Sims 2020-03-13`\n",
    "- USE: Get semi-parsed html for each statfox matchup page and clean into modeling input\n",
    "    - Treat each matchup page as one obs in RF model to asses feature importance\n",
    "    - Matchup data is pre-computed as of day of match, but needs lots of cleaning \n",
    "    - Includes opening and close money and total lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set datetime stamp in YYYYMMDD for all file outputs\n",
    "\n",
    "from datetime import date as dt\n",
    "YMD = dt.today().isoformat().replace('-','') + '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import all modules at top\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import codecs\n",
    "import os.path\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from urllib.request import urlopen\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FromPickleS3(bucketname, keyname):\n",
    "    \"\"\"Get dataframe from s3 and unpickle\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    pkldf = s3.get_object(Bucket=bucketname,Key=keyname)['Body'].read()\n",
    "    df = pickle.loads(pkldf)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Flatten2DLabel(obj, row_label, col_label):\n",
    "    \"\"\"Flattens a dataframe to single row and concatenates row and \n",
    "    column lables within the dataframe into one column name.                                 \n",
    "    \"\"\"\n",
    "    # Return all row indexes after last row_label\n",
    "    st_row = row_label[-1]+1\n",
    "    # Return all columns after last col_label\n",
    "    st_col = col_label[-1]+1\n",
    "\n",
    "\n",
    "    # Get row labels and concatenate\n",
    "    for x in row_label:\n",
    "        if x == row_label[0]:\n",
    "            cols = []\n",
    "            cols = obj.iloc[x][st_col:]\n",
    "        else:\n",
    "            cols = cols+'_'+obj.iloc[x][st_col:]\n",
    "\n",
    "    # Get column labels and concatenate\n",
    "    for x  in col_label:\n",
    "        if x == col_label[0]:\n",
    "            rows = []\n",
    "            rows = obj[st_row:][x]\n",
    "        else:\n",
    "            rows = rows+'_'+obj[st_row:][x]  \n",
    "\n",
    "    # Strip non alphanumeric\n",
    "    colstrip = cols.str.replace('[^\\w]','')\n",
    "    rowstrip = rows.str.replace('[^\\w]','')\n",
    "\n",
    "\n",
    "    obj_in = pd.DataFrame([])\n",
    "    for x in rowstrip.index:\n",
    "        for y in colstrip.index:\n",
    "            obj_tmp = pd.DataFrame([obj.iloc[x][y]], columns=[str(colstrip[y])+'_'+str(rowstrip[x])])\n",
    "            obj_in = pd.concat([obj_in,obj_tmp], axis=1)\n",
    "            \n",
    "    return obj_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanStatfoxScrape(obj):\n",
    "    \"\"\"Takes flattened sub-tables from statfox matchup\n",
    "    page and cleans and shortens feature names\n",
    "    \"\"\"\n",
    "    # Team names to V or H\n",
    "    obj.columns = obj.columns.str.replace(tm_v,'V_')\n",
    "    obj.columns = obj.columns.str.replace(tm_h,'H_')\n",
    "\n",
    "    # Shorten sub-table names\n",
    "    obj.columns = obj.columns.str.replace('CurrentSeasonPerformance', 'Overall')\n",
    "    obj.columns = obj.columns.str.replace('TeamHittingandFieldingStatistics','HitField')\n",
    "    obj.columns = obj.columns.str.replace('BullpenPitchingStatistics','Bullpen')\n",
    "    \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DuplicatedVarnames(df):\n",
    "    \"\"\"Return a dict of all variable names that \n",
    "    are duplicated in a given dataframe.\"\"\"\n",
    "    repeat_dict = {}\n",
    "    var_list = list(df) # list of varnames as strings\n",
    "    for varname in var_list:\n",
    "        # make a list of all instances of that varname\n",
    "        test_list = [v for v in var_list if v == varname] \n",
    "        # if more than one instance, report duplications in repeat_dict\n",
    "        if len(test_list) > 1: \n",
    "            repeat_dict[varname] = len(test_list)\n",
    "    return repeat_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get list of games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "glhead = pd.read_csv('GLHEADER.CSV',header=None)\n",
    "\n",
    "gms = pd.read_csv('20200313.matchup_list.2010-2018.csv.gz',\n",
    "                  header=0, \n",
    "                  names=glhead[0], \n",
    "                  low_memory=False\n",
    "                 )\n",
    "\n",
    "gms = gms[['date','team_h','team_v','score_h','score_v']]\n",
    "\n",
    "teams = pd.read_csv('TEAM_NAMES.CSV',\n",
    "                    header=0,\n",
    "                    index_col=['name1'],\n",
    "                    usecols=['name1','name3']\n",
    "                   )\n",
    "\n",
    "teams = teams['name3'].to_dict()\n",
    "\n",
    "gms['team_h'] = gms['team_h'].map(lambda x: teams[x.upper()])\n",
    "gms['team_v'] = gms['team_v'].map(lambda x: teams[x.upper()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dataframe from s3 and unpickle\n",
    "### Format and append each matchup together from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date        20100405\n",
      "team_h     LA ANGELS\n",
      "team_v     MINNESOTA\n",
      "score_h            6\n",
      "score_v            3\n",
      "Name: 0, dtype: object\n",
      "date            20100405\n",
      "team_h     CHI WHITE SOX\n",
      "team_v         CLEVELAND\n",
      "score_h                6\n",
      "score_v                0\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for x in gms.index[:2]:\n",
    "    print(gms.iloc[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-12 03:13:28.735535  -  statfox_DEV/20180625KANSASCITY.pkl  -  20600\n",
      "2020-03-12 03:14:24.561005  -  statfox_DEV/20180703COLORADO.pkl  -  20700\n",
      "2020-03-12 03:15:22.653832  -  statfox_DEV/20180710LAANGELS.pkl  -  20800\n",
      "2020-03-12 03:16:23.348677  -  statfox_DEV/20180721LAANGELS.pkl  -  20900\n",
      "2020-03-12 03:17:26.065971  -  statfox_DEV/20180728BOSTON.pkl  -  21000\n",
      "2020-03-12 03:18:31.321667  -  statfox_DEV/20180805LADODGERS.pkl  -  21100\n",
      "2020-03-12 03:19:39.274568  -  statfox_DEV/20180812LAANGELS.pkl  -  21200\n",
      "2020-03-12 03:20:52.291550  -  statfox_DEV/20180819OAKLAND.pkl  -  21300\n",
      "2020-03-12 03:22:06.514873  -  statfox_DEV/20180827SANFRANCISCO.pkl  -  21400\n",
      "2020-03-12 03:23:23.910832  -  statfox_DEV/20180903OAKLAND.pkl  -  21500\n",
      "2020-03-12 03:24:43.069145  -  statfox_DEV/20180911BOSTON.pkl  -  21600\n",
      "2020-03-12 03:26:06.577939  -  statfox_DEV/20180918OAKLAND.pkl  -  21700\n",
      "2020-03-12 03:27:34.141391  -  statfox_DEV/20180926SANFRANCISCO.pkl  -  21800\n"
     ]
    }
   ],
   "source": [
    "df_all = pd.DataFrame()\n",
    "\n",
    "\n",
    "# for x in gms.index[20600:]:\n",
    "    try:\n",
    "        \n",
    "        ### Parse date, team names, and score from games list\n",
    "        \n",
    "        dt = str(gms.loc[x,'date'])\n",
    "        tm_h = str(gms.loc[x,'team_h']).replace(' ','')                                                              \n",
    "        tm_v = str(gms.loc[x,'team_v']).replace(' ','')\n",
    "        sc_h = gms.loc[x,'score_h']\n",
    "        sc_v = gms.loc[x,'score_v']\n",
    "\n",
    "        ### Adjust URL if second game of double header\n",
    "        \n",
    "        if (x > 1) and (str(gms.loc[x-1,'date']) == dt) and (str(gms.loc[x-1,'team_h']).replace(' ','') == tm_h):\n",
    "            tm_h_url = tm_h+'2'\n",
    "        else:\n",
    "            tm_h_url = tm_h\n",
    "\n",
    "        ### Create dataframe from s3 pickle\n",
    "        \n",
    "        df = FromPickleS3(bucketname='scrapes-rawhtml-dev', keyname='statfox_DEV/'+dt+tm_h_url+'.pkl')\n",
    "\n",
    "        ### Show progress for my OCD\n",
    "        \n",
    "        if x%100 == 0:\n",
    "            timestamp = str(datetime.now())\n",
    "            print(timestamp, ' - ', 'statfox_DEV/'+dt+tm_h_url+'.pkl', ' - ',x)\n",
    "\n",
    "        ### Parse and concatenate all features together, by matchup\n",
    "\n",
    "        # 6:  Overall - board and line\n",
    "        # 11: Away - Current Season Performance\n",
    "        # 12: Away - Team Hitting and Fielding\n",
    "        # 13: Away - Bullpen Pitching \n",
    "        # 14: Home - Current Season Performance\n",
    "        # 15: Home - Team Hitting and Fielding\n",
    "        # 16: Home - Bullpen Pitching \n",
    "        df6  = df[6] \n",
    "        df11 = df[11]\n",
    "        df12 = df[12]\n",
    "        df13 = df[13]\n",
    "        df14 = df[14]\n",
    "        df15 = df[15]\n",
    "        df16 = df[16]\n",
    "\n",
    "        df6 = df6.transpose() # Make board and team labels first\n",
    "        df6_wd = Flatten2DLabel(df6, [1], [0,1])\n",
    "\n",
    "        df11_wd = Flatten2DLabel(df11, [0,1,2], [0])\n",
    "        df14_wd = Flatten2DLabel(df14, [0,1,2], [0])\n",
    "\n",
    "        df12_wd = Flatten2DLabel(df12, [0,1,2], [0])\n",
    "        df15_wd = Flatten2DLabel(df15, [0,1,2], [0])\n",
    "\n",
    "        df13_wd = Flatten2DLabel(df13, [0,1], [0])\n",
    "        df16_wd = Flatten2DLabel(df16, [0,1], [0])\n",
    "\n",
    "        ### Concatenate all sub-tables and clean labels\n",
    "\n",
    "        df_wd_all = pd.concat([df6_wd, df11_wd, df12_wd, df13_wd, df14_wd, df15_wd, df16_wd], axis=1)\n",
    "        df_wd_all = CleanStatfoxScrape(df_wd_all)\n",
    "\n",
    "        ### Add matchup index value date-board_h-board_v\n",
    "\n",
    "        board_h = df6[3].iloc[0]\n",
    "        board_v = df6[2].iloc[0]\n",
    "        df_wd_all.index = [int(dt)*1000000+int(board_h)*1000+int(board_v)]\n",
    "\n",
    "        ### Add team names and final scores\n",
    "\n",
    "        df_wd_all['tm_h'] = tm_h\n",
    "        df_wd_all['tm_v'] = tm_v\n",
    "        df_wd_all['sc_h'] = sc_h\n",
    "        df_wd_all['sc_v'] = sc_v\n",
    "\n",
    "        ### Drop columns duplicates (usually nan)\n",
    "\n",
    "        for x in DuplicatedVarnames(df_wd_all):\n",
    "            df_wd_all = df_wd_all.drop(list(df_wd_all.filter(regex=x)), axis=1)\n",
    "\n",
    "        ### Split features on hyphen if OU (over-under) or WL (win-loss)        \n",
    "\n",
    "        ou_cols = [col for col in df_wd_all.columns if 'OU' in col]\n",
    "\n",
    "        if ou_cols != []:\n",
    "            for x in ou_cols:\n",
    "                xO = x.replace('OU', 'O')\n",
    "                xU = x.replace('OU', 'U')\n",
    "                df_wd_all[[xO,xU]] = df_wd_all[x].str.split(\"-\",expand=True)    \n",
    "                df_wd_all = df_wd_all.drop(x, axis=1)\n",
    "\n",
    "        wl_cols = [col for col in df_wd_all.columns if 'WL' in col]\n",
    "\n",
    "        if wl_cols != []:\n",
    "            for x in wl_cols:\n",
    "                xW = x.replace('WL', 'W')\n",
    "                xL = x.replace('WL', 'L')\n",
    "                df_wd_all[[xW,xL]] = df_wd_all[x].str.split(\"-\",expand=True)    \n",
    "                df_wd_all = df_wd_all.drop(x, axis=1)    \n",
    "\n",
    "        ### Create target variable (home win)\n",
    "\n",
    "        df_wd_all['win_h'] = np.where(df_wd_all['sc_h'] > df_wd_all['sc_v'], 1, 0)\n",
    "\n",
    "        ### Combine feature rows\n",
    "\n",
    "        df_all = pd.concat([df_all, df_wd_all], ignore_index=False, sort=True)\n",
    "    \n",
    "    ### Print exception to log and go to next iteration (URL)\n",
    "    \n",
    "    except Exception as exc:\n",
    "        timestamp = str(datetime.now())\n",
    "        exc = str(exc)\n",
    "        excstamp = timestamp+' - '+exc+' - '+dt+tm_h_url\n",
    "        print(excstamp, file=open(file_l, 'a'))\n",
    "        continue\n",
    "    \n",
    "    except IndexError as err:\n",
    "        timestamp = str(datetime.now())\n",
    "        excstamp = timestamp+' - '+err+' - '+dt+tm_h_url\n",
    "        print(excstamp, file=open(file_l, 'a'))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save cleaned to gzip tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1267, 581)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv(YMD+'statfox_clean_scrapes.2010-2018.csv.gz', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_in1 = '20200310.skr_statfox.2010-2018.0.7400.tsv.gz'\n",
    "file_in2 = '20200310.skr_statfox.2010-2018.7400.10000.tsv.gz'\n",
    "file_in3 = '20200310.skr_statfox.2010-2018.10000.12000.tsv.gz'\n",
    "file_in4 = '20200310.skr_statfox.2010-2018.12000.16000.tsv.gz'\n",
    "file_in5 = '20200310.skr_statfox.2010-2018.16000.20600.tsv.gz'\n",
    "file_in6 = '20200310.skr_statfox.2010-2018.20600.tsv.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get statfox_clean_scrapes from s3 since not local anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "s3.list_objects(Bucket='scrapes-rawhtml-dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.download_file(YMD+'statfox_features.tsv.gz', \n",
    "               Bucket='scrapes-rawhtml-dev', \n",
    "               Key='statfox/'+YMD+'statfox_features.tsv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import features, moneylines, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '20200310.skr_statfox.2010-2018.0.7400.tsv.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7ba0a5f303a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### Import chunks and append\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_in1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_in2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_in3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '20200310.skr_statfox.2010-2018.0.7400.tsv.gz'"
     ]
    }
   ],
   "source": [
    "### Import chunks and append\n",
    "\n",
    "df1 = pd.read_csv(file_in1, sep='\\t', low_memory=False)\n",
    "df2 = pd.read_csv(file_in2, sep='\\t', low_memory=False)\n",
    "df3 = pd.read_csv(file_in3, sep='\\t', low_memory=False)\n",
    "df4 = pd.read_csv(file_in4, sep='\\t', low_memory=False)\n",
    "df5 = pd.read_csv(file_in5, sep='\\t', low_memory=False)\n",
    "df6 = pd.read_csv(file_in6, sep='\\t', low_memory=False)\n",
    "\n",
    "df = pd.concat([df1, df2, df3, df4, df5, df6], ignore_index=False, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clear useless dataframes\n",
    "\n",
    "df1 = []\n",
    "df2 = []\n",
    "df3 = []\n",
    "df4 = []\n",
    "df5 = []\n",
    "df6 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Give proper name to the match index\n",
    "\n",
    "df['matchidx'] = df['Unnamed: 0']\n",
    "df = df.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dedupe on matchup index (YYYYMMDDHHHVVV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(['matchidx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add year and month variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month(x):\n",
    "    \"\"\"Take match index YYYYMMDDHHHVVV and return the month of game\n",
    "    \"\"\"\n",
    "    flr = math.floor(x/100000000)\n",
    "    flrmod = flr%100\n",
    "    return str(flrmod)\n",
    "\n",
    "def get_year(x):\n",
    "    \"\"\"Take match index YYYYMMDDHHHVVV and return the year of game\n",
    "    \"\"\"\n",
    "    flr = math.floor(x/10000000000)\n",
    "    return str(flr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month'] = df['matchidx'].apply(get_month)\n",
    "df['year'] = df['matchidx'].apply(get_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['matchidx','year','month']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up OU Totals, OSB, and DP features\n",
    "- This should really be in skr_statfox_matchups.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split by a comma then drop Ov and Un text\n",
    "\n",
    "ou_cols = [col for col in df.columns if '_Total' in col]\n",
    "\n",
    "if ou_cols != []:\n",
    "    for x in ou_cols:\n",
    "        xO = x.replace('_Total', '_Tot')\n",
    "        xU = x.replace('_Total', '_TotLn')\n",
    "        \n",
    "        df[[xO,xU]] = df[x].str.split(\",\", expand=True)    \n",
    "        df[xO] = df[xO].str.replace(\"Ov \",\"\")\n",
    "        df[xO] = df[xO].str.replace(\"Un \",\"\")\n",
    "        df = df.drop(x, axis=1)\n",
    "    \n",
    "new_ou_cols = [col for col in df.columns if '_Tot' in col]    \n",
    "df[new_ou_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove parantheses from numeric values\n",
    "\n",
    "osb_cols = [col for col in df.columns if '_OSB_' in col]\n",
    "dp_cols = [col for col in df.columns if '_DP_' in col]\n",
    "paren_cols = osb_cols + dp_cols\n",
    "\n",
    "if paren_cols != []:\n",
    "    for x in paren_cols:\n",
    "        df[x] = df[x].str.replace(\"(\",\"\")\n",
    "        df[x] = df[x].str.replace(\")\",\"\")\n",
    "        \n",
    "df[paren_cols].head()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove percent signs\n",
    "\n",
    "perc = [col for col in df.columns if '_Pct_' in col]\n",
    "\n",
    "if perc != []:\n",
    "    for x in perc:\n",
    "        df[x] = df[x].str.replace(\"%\",\"\")\n",
    "        df[x] = df[x].str.replace(\" \",\"\")\n",
    "\n",
    "df[perc].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate features, targets, and misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [col for col in df.columns if 'Latest_Line' in col]\n",
    "teams = ['tm_h', 'tm_v']\n",
    "scores = ['sc_h','sc_v']\n",
    "feats = ['win_h']\n",
    "# drop = pd.concat([lines, totals, scores, feats])\n",
    "drop = lines+scores+feats\n",
    "drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = df.drop(drop, axis=1).reset_index(drop=True)\n",
    "df_targ = pd.to_numeric(df['win_h'], errors='coerce').reset_index(drop=True)\n",
    "df_openline = pd.to_numeric(df['H__Opening_Line'], errors='coerce').reset_index(drop=True)\n",
    "df_lateline = pd.to_numeric(df['H__Latest_Line'], errors='coerce').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Clean\n",
    "- Try to convert object to numeric\n",
    "- If except: convert object to binary dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tonumeric(x, coercecols):\n",
    "    \"\"\"Clean features to get everything numeric.\n",
    "    1) Strip percent sign, space, and commas\n",
    "    2) Convert objects to numeric if possible\n",
    "    3) Else, convert objects to dummies\n",
    "    \"\"\"\n",
    "#     output = pd.DataFrame()\n",
    "    \n",
    "    for col in coercecols:\n",
    "        if col not in x.columns:\n",
    "            raise ValueError\n",
    "    \n",
    "    for col, col_data in x.iteritems():\n",
    "        \n",
    "        if col_data.dtype == object and col in coercecols:\n",
    "            \n",
    "            col_data = pd.to_numeric(col_data, errors='coerce')\n",
    "            x = x.drop(col, axis=1)\n",
    "            \n",
    "        elif col_data.dtype == object:\n",
    "                \n",
    "            try:\n",
    "                col_data = pd.to_numeric(col_data)\n",
    "                x = x.drop(col, axis=1)\n",
    "                \n",
    "            except:\n",
    "                col_data = pd.get_dummies(col_data, prefix=col)\n",
    "                x = x.drop(col, axis=1)\n",
    "            \n",
    "        x = pd.concat([x, col_data], axis=1)\n",
    "    \n",
    "    ### Keep first of all columns then drop duplicates\n",
    "\n",
    "    Cols = list(x.columns)\n",
    "    for i,item in enumerate(x.columns):\n",
    "        if item in x.columns[:i]: Cols[i] = \"toDROP\"\n",
    "    x.columns = Cols\n",
    "    x = x.drop(\"toDROP\",1)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_nan(x, fill_strategy='mean'):\n",
    "def preprocess_nan(z):\n",
    "    \"\"\"Process NaNs in a dataframe to prepare for RF or other model.\n",
    "    1) Creates dummy columns for each existing column with > 1 NaN\n",
    "    2) Fill NaN in existing column with desired strategy (see scikit SimpleImputer)\n",
    "    \"\"\"\n",
    "    for col in z.columns:\n",
    "        if z[col].isnull().any().any():\n",
    "            nancol = col+'_NaN'\n",
    "            z[nancol] = [1 if s == True else 0 for s in z[col].isnull()]\n",
    "        \n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    x_out = pd.DataFrame(imp.fit_transform(z), columns=z.columns)\n",
    "    \n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_num = preprocess_tonumeric(df_feat, coercecols=['H__Opening_Line', 'V__Opening_Line'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_num[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert open and close moneylines to probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def american_to_probability(x):\n",
    "    \"\"\"Turns American +/- odds into probability 0 to 1 exclusive\n",
    "    NOTE: Returns 0 if missing since 0 and 1 are impossible from lines\n",
    "    \"\"\"\n",
    "    if x < -99:\n",
    "        num = abs(x)\n",
    "        pr = num/(100+num)\n",
    "        return pr\n",
    "\n",
    "    elif x >= 100:\n",
    "        num = x\n",
    "        pr = 100/(100+num)\n",
    "        return pr\n",
    "    \n",
    "    elif x == None:\n",
    "        pr = 0\n",
    "\n",
    "#     else:\n",
    "#         print('Error: No sign found in betting line string')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert moneylines to probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Separate open/close moneylines from features\n",
    "\n",
    "df_lateline_prob = df_lateline.apply(american_to_probability)\n",
    "# df_lateline_prob = df_lateline_prob[lambda x: (x > 0) & (x < 1)]\n",
    "df_openline_prob = df_openline.apply(american_to_probability)\n",
    "# df_openline_prob = df_openline_prob[lambda x: (x > 0) & (x < 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert line features to probability\n",
    "\n",
    "lines = ['H__Opening_Line', 'V__Opening_Line', 'H__Opening_TotLn', 'V__Opening_TotLn']\n",
    "\n",
    "for col in lines:\n",
    "    newcol = col + 'Pr'\n",
    "    df_feat_num[newcol] = df_feat_num[col].apply(american_to_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check for any Opening lines (should have probabilities created *Pr*)\n",
    "\n",
    "df_feat_num[[col for col in df_feat_num.columns if 'Opening' in col]].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sample weighting based on payouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_to_weight(x):\n",
    "    \"\"\"Compute the payout of correct prediction to use as weights\n",
    "    in modeling, assuming $1.00 bet each time. \n",
    "        This should be a good\n",
    "    proxy for profit/loss since payouts change but loss is always equal\n",
    "    to bet ($1.00 in this case).\n",
    "    \"\"\"\n",
    "    win_h = x['win_h']\n",
    "    pr_h = x['H__Opening_LinePr']\n",
    "    pr_v = x['V__Opening_LinePr']\n",
    "    \n",
    "    if win_h == 0:\n",
    "        wt = 1 / pr_v\n",
    "    elif win_h == 1:\n",
    "        wt = 1 / pr_h\n",
    "    else:\n",
    "        wt = np.nan\n",
    "        \n",
    "    return wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_feat_num[['win_h', 'H__Opening_LinePr', 'V__Opening_LinePr']]\n",
    "pd.concat([test, test.apply(probability_to_weight, axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing values and create dummy for missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H_Bullpen_BB_AllGames</th>\n",
       "      <th>H_Bullpen_BB_HomeGames</th>\n",
       "      <th>H_Bullpen_BSV_AllGames</th>\n",
       "      <th>H_Bullpen_BSV_HomeGames</th>\n",
       "      <th>H_Bullpen_ERA_AllGames</th>\n",
       "      <th>H_Bullpen_ERA_HomeGames</th>\n",
       "      <th>H_Bullpen_ER_AllGames</th>\n",
       "      <th>H_Bullpen_ER_HomeGames</th>\n",
       "      <th>H_Bullpen_HR_AllGames</th>\n",
       "      <th>H_Bullpen_HR_HomeGames</th>\n",
       "      <th>...</th>\n",
       "      <th>H__Opening_Tot_NaN</th>\n",
       "      <th>H__Opening_TotLn_NaN</th>\n",
       "      <th>V__Latest_Tot_NaN</th>\n",
       "      <th>V__Latest_TotLn_NaN</th>\n",
       "      <th>V__Opening_Tot_NaN</th>\n",
       "      <th>V__Opening_TotLn_NaN</th>\n",
       "      <th>H__Opening_LinePr_NaN</th>\n",
       "      <th>V__Opening_LinePr_NaN</th>\n",
       "      <th>H__Opening_TotLnPr_NaN</th>\n",
       "      <th>V__Opening_TotLnPr_NaN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>228.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.42</td>\n",
       "      <td>4.69</td>\n",
       "      <td>253.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>190.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.06</td>\n",
       "      <td>4.41</td>\n",
       "      <td>212.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   H_Bullpen_BB_AllGames  H_Bullpen_BB_HomeGames  H_Bullpen_BSV_AllGames  \\\n",
       "0                  228.0                   117.0                    17.0   \n",
       "1                  190.0                   107.0                    18.0   \n",
       "\n",
       "   H_Bullpen_BSV_HomeGames  H_Bullpen_ERA_AllGames  H_Bullpen_ERA_HomeGames  \\\n",
       "0                      9.0                    4.42                     4.69   \n",
       "1                      8.0                    4.06                     4.41   \n",
       "\n",
       "   H_Bullpen_ER_AllGames  H_Bullpen_ER_HomeGames  H_Bullpen_HR_AllGames  \\\n",
       "0                  253.0                   138.0                   56.0   \n",
       "1                  212.0                   126.0                   49.0   \n",
       "\n",
       "   H_Bullpen_HR_HomeGames  ...  H__Opening_Tot_NaN  H__Opening_TotLn_NaN  \\\n",
       "0                    30.0  ...                 0.0                   0.0   \n",
       "1                    25.0  ...                 0.0                   0.0   \n",
       "\n",
       "   V__Latest_Tot_NaN  V__Latest_TotLn_NaN  V__Opening_Tot_NaN  \\\n",
       "0                0.0                  0.0                 0.0   \n",
       "1                0.0                  0.0                 0.0   \n",
       "\n",
       "   V__Opening_TotLn_NaN  H__Opening_LinePr_NaN  V__Opening_LinePr_NaN  \\\n",
       "0                   0.0                    0.0                    0.0   \n",
       "1                   0.0                    0.0                    0.0   \n",
       "\n",
       "   H__Opening_TotLnPr_NaN  V__Opening_TotLnPr_NaN  \n",
       "0                     0.0                     0.0  \n",
       "1                     0.0                     0.0  \n",
       "\n",
       "[2 rows x 1127 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat_fill = preprocess_nan(df_feat_num)\n",
    "df_feat_fill[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: object)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Check for any Closing lines (should be none)\n",
    "\n",
    "df_feat_fill[[col for col in df_feat_fill.columns if 'Closing' in col]].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check of any object cols (should be none)\n",
    "\n",
    "for col, col_data in df_feat_fill.iteritems():\n",
    "    if col_data.dtype == object:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check if any nan entries (should be none)\n",
    "\n",
    "for col in df_feat_fill.columns:\n",
    "    if df_feat_fill[col].isna().any():\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to tsv.gz then s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_fill.to_csv(YMD+'statfox_features.tsv.gz', sep='\\t')\n",
    "df_targ.to_csv(YMD+'statfox_target.tsv.gz', sep='\\t', header=False)\n",
    "df_lateline_prob.to_csv(YMD+'statfox_lateline_prob.tsv.gz', sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "s3.upload_file(YMD+'statfox_features.tsv.gz', \n",
    "               Bucket='scrapes-rawhtml-dev', \n",
    "               Key='statfox/'+YMD+'statfox_features.tsv.gz')\n",
    "\n",
    "s3.upload_file(YMD+'statfox_target.tsv.gz', \n",
    "               Bucket='scrapes-rawhtml-dev', \n",
    "               Key='statfox/'+YMD+'statfox_target.tsv.gz')\n",
    "\n",
    "s3.upload_file(YMD+'statfox_lateline_prob.tsv.gz', \n",
    "               Bucket='scrapes-rawhtml-dev', \n",
    "               Key='statfox/'+YMD+'statfox_lateline_prob.tsv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17573, 1127)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat_fill.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17573,)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_targ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        Series\n",
       "\u001b[0;31mString form:\u001b[0m\n",
       "0        1\n",
       "           1        1\n",
       "           2        0\n",
       "           3        0\n",
       "           4        1\n",
       "           5        1\n",
       "           6        1\n",
       "           7        0\n",
       "           8         <...> 0\n",
       "           17568    1\n",
       "           17569    1\n",
       "           17570    1\n",
       "           17571    0\n",
       "           17572    1\n",
       "           Name: win_h, Length: 17573, dtype: int64\n",
       "\u001b[0;31mLength:\u001b[0m      17573\n",
       "\u001b[0;31mFile:\u001b[0m        ~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/series.py\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "One-dimensional ndarray with axis labels (including time series).\n",
       "\n",
       "Labels need not be unique but must be a hashable type. The object\n",
       "supports both integer- and label-based indexing and provides a host of\n",
       "methods for performing operations involving the index. Statistical\n",
       "methods from ndarray have been overridden to automatically exclude\n",
       "missing data (currently represented as NaN).\n",
       "\n",
       "Operations between Series (+, -, /, *, **) align values based on their\n",
       "associated index values-- they need not be the same length. The result\n",
       "index will be the sorted union of the two indexes.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "data : array-like, Iterable, dict, or scalar value\n",
       "    Contains data stored in Series.\n",
       "\n",
       "    .. versionchanged :: 0.23.0\n",
       "       If data is a dict, argument order is maintained for Python 3.6\n",
       "       and later.\n",
       "\n",
       "index : array-like or Index (1d)\n",
       "    Values must be hashable and have the same length as `data`.\n",
       "    Non-unique index values are allowed. Will default to\n",
       "    RangeIndex (0, 1, 2, ..., n) if not provided. If both a dict and index\n",
       "    sequence are used, the index will override the keys found in the\n",
       "    dict.\n",
       "dtype : str, numpy.dtype, or ExtensionDtype, optional\n",
       "    dtype for the output Series. If not specified, this will be\n",
       "    inferred from `data`.\n",
       "    See the :ref:`user guide <basics.dtypes>` for more usages.\n",
       "copy : bool, default False\n",
       "    Copy input data.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_targ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
