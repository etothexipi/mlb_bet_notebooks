{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15 - Data Engineering\n",
    "\"Python for DevOps\" - Noah Gift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the tools of data engineering outline themselves. These tools include small data tasks like reading and writing files, using `pickle`, using `JSON`, and writing and reading `YAML` files. Being able to master these formats is critical to be the type of automate who can tackle any task and turn it into a script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"containers.txt\", \"w\") as file_to_write:\n",
    "    file_to_write.write(\"Pod\\n\")\n",
    "    file_to_write.write(\"Service\\n\")\n",
    "    file_to_write.write(\"Volume\\n\")\n",
    "    file_to_write.write(\"Namespace\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pod\n",
      "Service\n",
      "Volume\n",
      "Namespace\n"
     ]
    }
   ],
   "source": [
    "!cat containers.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pod\\n', 'Service\\n', 'Volume\\n', 'Namespace\\n']\n"
     ]
    }
   ],
   "source": [
    "with open(\"containers.txt\") as file_to_read:\n",
    "    lines = file_to_read.readlines()\n",
    "    print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Pipeline to Read and Process Lines\n",
    ">NOTE: maybe use to parse HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_lazily():\n",
    "    \"\"\"Uses generator to lazily process file\"\"\"\n",
    "    \n",
    "    with open(\"containers.txt\") as file_to_read:\n",
    "        for line in file_to_read.readlines():\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Next, this generator is used to create a pipeline to perform operations line by line. This example converts line to lowercase string. This is a very efficient way to chain actions together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create generator object\n",
    "pipeline = process_file_lazily()\n",
    "# convert to lowercase\n",
    "lowercase = (line.lower() for line in pipeline)\n",
    "# print first processed line\n",
    "print(next(lowercase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This means that files which are infinite could still be processed because the code exits when it find a condition. A generator pipeline could look for a customer ID and then exit the processing at the first occurrence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using YAML\n",
    "> YAML is becoming an emerging standard for config files. It is a human-readable data serialization format that is a superset of JSON. It is often used because there is a need for a configuration language that allows rapid iteration when interacting with highly automated systems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubernetes_components = {\n",
    "    \"Pod\": \"Basic building block of Kubernetes.\",\n",
    "    \"Service\": \"An abstraction for dealing with Pods.\",\n",
    "    \"Volume\": \"A directory accessible to containers in a Pod.\",\n",
    "    \"Namespaces\": \"A way to divide cluster resources between users.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kubernetes_info.yaml\", \"w\") as yaml_to_write:\n",
    "    yaml.safe_dump(kubernetes_components, yaml_to_write, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespaces: A way to divide cluster resources between users.\n",
      "Pod: Basic building block of Kubernetes.\n",
      "Service: An abstraction for dealing with Pods.\n",
      "Volume: A directory accessible to containers in a Pod.\n"
     ]
    }
   ],
   "source": [
    "!cat kubernetes_info.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The takeaway is that it makes it trivial to serialize a Python data structure into a format that is easy to edit and iterate on. Reading the file back is just two lines of code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kubernetes_info.yaml\", \"rb\") as yaml_to_read:\n",
    "    result = yaml.safe_load(yaml_to_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'Namespaces': 'A way to divide cluster resources between users.',\n",
      "    'Pod': 'Basic building block of Kubernetes.',\n",
      "    'Service': 'An abstraction for dealing with Pods.',\n",
      "    'Volume': 'A directory accessible to containers in a Pod.'}\n"
     ]
    }
   ],
   "source": [
    "# Pretty print YAML file\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
