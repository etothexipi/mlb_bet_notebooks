{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape StatFox for Current and Historical Matchup Stats\n",
    "`Notebooks/skr_statfox.ipynb`\n",
    "###### `BY: Jonathan Sims` \n",
    "###### `CREATED: 2019-06-15`\n",
    "- GOAL: Scrape daily to build up historical matchup tables\n",
    "- USE: Treat each matchup page as one obs in RF model to asses feature importance\n",
    "    - Useful before spending time building up game/pitch/player-level count data \n",
    "    - Compare odds here with odds from VegasInsider to look for odds movement features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ToPickleS3(obj, bucketname, keyname):\n",
    "    \"\"\"Pickle dataframe and put to s3 bucket in site name folder\n",
    "    i.e. 'statfox/'\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    serializedListObject = pickle.dumps(obj)\n",
    "    s3.put_object(Bucket=bucketname,Key=keyname,Body=serializedListObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FromPickleS3(bucketname, keyname):\n",
    "    \"\"\"Get dataframe from s3 and unpickle\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    pkldf = s3.get_object(Bucket=bucketname,Key=keyname)['Body'].read()\n",
    "    df = pickle.loads(pkldf)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Flatten2DLabel(obj, row_label, col_label):\n",
    "    \"\"\"Flattens a dataframe to single row and concatenates row and \n",
    "    column lables within the dataframe into one column name.                                 \n",
    "    \"\"\"\n",
    "    # Return all row indexes after last row_label\n",
    "    st_row = row_label[-1]+1\n",
    "    # Return all columns after last col_label\n",
    "    st_col = col_label[-1]+1\n",
    "\n",
    "\n",
    "    # Get row labels and concatenate\n",
    "    for x in row_label:\n",
    "        if x == row_label[0]:\n",
    "            cols = []\n",
    "            cols = obj.iloc[x][st_col:]\n",
    "        else:\n",
    "            cols = cols+'_'+obj.iloc[x][st_col:]\n",
    "\n",
    "    # Get column labels and concatenate\n",
    "    for x  in col_label:\n",
    "        if x == col_label[0]:\n",
    "            rows = []\n",
    "            rows = obj[st_row:][x]\n",
    "        else:\n",
    "            rows = rows+'_'+obj[st_row:][x]  \n",
    "\n",
    "    # Strip non alphanumeric\n",
    "    colstrip = cols.str.replace('[^\\w]','')\n",
    "    rowstrip = rows.str.replace('[^\\w]','')\n",
    "\n",
    "\n",
    "    obj_in = pd.DataFrame([])\n",
    "    for x in rowstrip.index:\n",
    "        for y in colstrip.index:\n",
    "            obj_tmp = pd.DataFrame([obj.iloc[x][y]], columns=[str(colstrip[y])+'_'+str(rowstrip[x])])\n",
    "            obj_in = pd.concat([obj_in,obj_tmp], axis=1)\n",
    "            \n",
    "    return obj_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanStatfoxScrape(obj):\n",
    "    \"\"\"Takes flattened sub-tables from statfox matchup\n",
    "    page and cleans and shortens feature names\n",
    "    \"\"\"\n",
    "    obj_in = obj\n",
    "    \n",
    "    # Team names to V or H\n",
    "    obj_in.columns = obj_in.columns.str.replace(tm_v,'V_')\n",
    "    obj_in.columns = obj_in.columns.str.replace(tm_h,'H_')\n",
    "\n",
    "    # Shorten sub-table names\n",
    "    obj_in.columns = obj_in.columns.str.replace('CurrentSeasonPerformance', 'Overall')\n",
    "    obj_in.columns = obj_in.columns.str.replace('TeamHittingandFieldingStatistics','HitField')\n",
    "    obj_in.columns = obj_in.columns.str.replace('BullpenPitchingStatistics','Bullpen')\n",
    "    \n",
    "    return obj_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get list of games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "glhead = pd.read_csv('GLHEADER.CSV',header=None)\n",
    "gms = pd.read_csv('GL2018.CSV',header=0,names=list(glhead[0]))\n",
    "gms = gms[['date','team_h','team_v','score_h','score_v']]\n",
    "\n",
    "teams = pd.read_csv('TEAM_NAMES.CSV',header=0,index_col=['name1'],usecols=['name1','name3'])\n",
    "teams = teams['name3'].to_dict()\n",
    "\n",
    "gms['team_h'] = gms['team_h'].map(lambda x: teams[x.upper()])\n",
    "gms['team_v'] = gms['team_v'].map(lambda x: teams[x.upper()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visit each page in games list, scrape, and send to s3 as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# x is game number in games list\n",
    "for x in range(len(gms)):\n",
    "# for x in range(40,46):\n",
    "    \n",
    "    \n",
    "    # Parse date, team names, and score from games list\n",
    "    dt = str(gms.loc[x,'date'])\n",
    "    tm_h = str(gms.loc[x,'team_h']).replace(' ','')                                                              \n",
    "    tm_v = str(gms.loc[x,'team_v']).replace(' ','')\n",
    "    sc_h = gms.loc[x,'score_h']\n",
    "    sc_v = gms.loc[x,'score_v']\n",
    "\n",
    "\n",
    "    # Adjust URL if second game of double header\n",
    "    if (x > 1) and (str(gms.loc[x-1,'date']) == dt) and (str(gms.loc[x-1,'team_h']).replace(' ','') == tm_h):\n",
    "        tm_h = tm_h+'2'\n",
    "\n",
    "    \n",
    "    # Set game and s3 bucket, URL, and file name\n",
    "    game = dt+tm_h\n",
    "    keyname = folder+game+'.pkl'\n",
    "    url = 'http://foxsheets.statfoxsports.com/foxsheets.aspx?s=mlb&g='+dt+tm_h+'&r=at'\n",
    "    \n",
    "    \n",
    "    # Parse HTML\n",
    "    html = urlopen(url)\n",
    "    bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "\n",
    "\n",
    "    # Get all tables from page\n",
    "    nameList = bs.findAll('td', {'class':['matchupBorder']})\n",
    "\n",
    "     \n",
    "    # Save each table to a dataframe and pickle\n",
    "    namestr = str(nameList)\n",
    "    df = pd.read_html(namestr)\n",
    "    ToPickleS3(df, bucketname='scrapes-rawhtml-dev', keyname='statfox_DEV/'+dt+tm_h+'.pkl')\n",
    "    \n",
    "    # Checkpoint\n",
    "    if x%100 == 0:\n",
    "        print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get dataframe from s3 and unpickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format and append each matchup together from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Number of manager items must equal union of block items\n# manager items: 338, # tot_items: 354",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-0bfb25bb5aa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m#### Combine feature rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mdf_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_wd_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   6690\u001b[0m         return concat(to_concat, ignore_index=ignore_index,\n\u001b[1;32m   6691\u001b[0m                       \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6692\u001b[0;31m                       sort=sort)\n\u001b[0m\u001b[1;32m   6693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6694\u001b[0m     def join(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    227\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                        copy=copy, sort=sort)\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    424\u001b[0m             new_data = concatenate_block_managers(\n\u001b[1;32m    425\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m                 copy=self.copy)\n\u001b[0m\u001b[1;32m    427\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m   2063\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2065\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, blocks, axes, do_integrity_check)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_integrity_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_verify_integrity\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m                                  \u001b[0;34m'block items\\n# manager items: {0}, # '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                                  'tot_items: {1}'.format(\n\u001b[0;32m--> 316\u001b[0;31m                                      len(self.items), tot_items))\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     def apply(self, f, axes=None, filter=None, do_integrity_check=False,\n",
      "\u001b[0;31mAssertionError\u001b[0m: Number of manager items must equal union of block items\n# manager items: 338, # tot_items: 354"
     ]
    }
   ],
   "source": [
    "df_all = pd.DataFrame([])\n",
    "\n",
    "for x in range(45,47):\n",
    "    # Parse date, team names, and score from games list\n",
    "    dt = str(gms.loc[x,'date'])\n",
    "    tm_h = str(gms.loc[x,'team_h']).replace(' ','')                                                              \n",
    "    tm_v = str(gms.loc[x,'team_v']).replace(' ','')\n",
    "    sc_h = gms.loc[x,'score_h']\n",
    "    sc_v = gms.loc[x,'score_v']\n",
    "\n",
    "    # Adjust URL if second game of double header\n",
    "    if (x > 1) and (str(gms.loc[x-1,'date']) == dt) and (str(gms.loc[x-1,'team_h']).replace(' ','') == tm_h):\n",
    "        tm_h = tm_h+'2'\n",
    "    \n",
    "    # Create dataframe from s3 pickle\n",
    "    df = FromPickleS3(bucketname='scrapes-rawhtml-dev', keyname='statfox_DEV/'+dt+tm_h+'.pkl')\n",
    "\n",
    "#### Parse and concatenate all features together, by matchup\n",
    "\n",
    "    # 6:  Overall - board and line\n",
    "    # 11: Away - Current Season Performance\n",
    "    # 12: Away - Team Hitting and Fielding\n",
    "    # 13: Away - Bullpen Pitching \n",
    "    # 14: Home - Current Season Performance\n",
    "    # 15: Home - Team Hitting and Fielding\n",
    "    # 16: Home - Bullpen Pitching \n",
    "    df6  = df[6] \n",
    "    df11 = df[11]\n",
    "    df12 = df[12]\n",
    "    df13 = df[13]\n",
    "    df14 = df[14]\n",
    "    df15 = df[15]\n",
    "    df16 = df[16]\n",
    "\n",
    "    df6 = df6.transpose() # Make board and team labels first\n",
    "    df6_wd = Flatten2DLabel(df6, [0,1], [0,1])\n",
    "\n",
    "    df11_wd = Flatten2DLabel(df11, [0,1,2], [0])\n",
    "    df14_wd = Flatten2DLabel(df14, [0,1,2], [0])\n",
    "\n",
    "    df12_wd = Flatten2DLabel(df12, [0,1,2], [0])\n",
    "    df15_wd = Flatten2DLabel(df15, [0,1,2], [0])\n",
    "\n",
    "    df13_wd = Flatten2DLabel(df13, [0,1], [0])\n",
    "    df16_wd = Flatten2DLabel(df16, [0,1], [0])\n",
    "\n",
    "#### Concatenate all sub-tables and clean labels\n",
    "\n",
    "    df_wd_all = pd.concat([df6_wd, df11_wd, df12_wd, df13_wd, df14_wd, df15_wd, df16_wd], axis=1)\n",
    "    df_wd_all = CleanStatfoxScrape(df_wd_all)\n",
    "\n",
    "#### Combine feature rows\n",
    "\n",
    "    df_all = df_all.append(df_wd_all, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "append() got an unexpected keyword argument 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e777d5718b98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_wd_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: append() got an unexpected keyword argument 'axis'"
     ]
    }
   ],
   "source": [
    "df_all = df_all.append(df_wd_all, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mdf_wd_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Append rows of `other` to the end of caller, returning a new object.\n",
       "\n",
       "Columns in `other` that are not in the caller are added as new columns.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "other : DataFrame or Series/dict-like object, or list of these\n",
       "    The data to append.\n",
       "ignore_index : boolean, default False\n",
       "    If True, do not use the index labels.\n",
       "verify_integrity : boolean, default False\n",
       "    If True, raise ValueError on creating index with duplicates.\n",
       "sort : boolean, default None\n",
       "    Sort columns if the columns of `self` and `other` are not aligned.\n",
       "    The default sorting is deprecated and will change to not-sorting\n",
       "    in a future version of pandas. Explicitly pass ``sort=True`` to\n",
       "    silence the warning and sort. Explicitly pass ``sort=False`` to\n",
       "    silence the warning and not sort.\n",
       "\n",
       "    .. versionadded:: 0.23.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       "appended : DataFrame\n",
       "\n",
       "See Also\n",
       "--------\n",
       "pandas.concat : General function to concatenate DataFrame, Series\n",
       "    or Panel objects.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "If a list of dict/series is passed and the keys are all contained in\n",
       "the DataFrame's index, the order of the columns in the resulting\n",
       "DataFrame will be unchanged.\n",
       "\n",
       "Iteratively appending rows to a DataFrame can be more computationally\n",
       "intensive than a single concatenate. A better solution is to append\n",
       "those rows to a list and then concatenate the list with the original\n",
       "DataFrame all at once.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "\n",
       ">>> df = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\n",
       ">>> df\n",
       "   A  B\n",
       "0  1  2\n",
       "1  3  4\n",
       ">>> df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB'))\n",
       ">>> df.append(df2)\n",
       "   A  B\n",
       "0  1  2\n",
       "1  3  4\n",
       "0  5  6\n",
       "1  7  8\n",
       "\n",
       "With `ignore_index` set to True:\n",
       "\n",
       ">>> df.append(df2, ignore_index=True)\n",
       "   A  B\n",
       "0  1  2\n",
       "1  3  4\n",
       "2  5  6\n",
       "3  7  8\n",
       "\n",
       "The following, while not recommended methods for generating DataFrames,\n",
       "show two ways to generate a DataFrame from multiple data sources.\n",
       "\n",
       "Less efficient:\n",
       "\n",
       ">>> df = pd.DataFrame(columns=['A'])\n",
       ">>> for i in range(5):\n",
       "...     df = df.append({'A': i}, ignore_index=True)\n",
       ">>> df\n",
       "   A\n",
       "0  0\n",
       "1  1\n",
       "2  2\n",
       "3  3\n",
       "4  4\n",
       "\n",
       "More efficient:\n",
       "\n",
       ">>> pd.concat([pd.DataFrame([i], columns=['A']) for i in range(5)],\n",
       "...           ignore_index=True)\n",
       "   A\n",
       "0  0\n",
       "1  1\n",
       "2  2\n",
       "3  3\n",
       "4  4\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_wd_all.append?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `20200221`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MySQL Connector needs Python float, not numpy float64"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "def float64_to_float(x):\n",
    "    try:\n",
    "        if x.dtype == 'float64':\n",
    "            return float(x)\n",
    "    except:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert American betting lines to probability value satisfying E[x]=0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "def american_to_probability(x):\n",
    "    \"\"\"\n",
    "    Turns American +/- odds into probability 0 to 1 inclusive\n",
    "    \"\"\"\n",
    "    if x.find('-') > -1 and x.find('+') > -1:\n",
    "        print('Error: + and - signs found in betting line string') \n",
    "\n",
    "    elif x.find('-') > -1:\n",
    "        num = int(x.replace(' ','').replace('-',''))\n",
    "        if num < 100:\n",
    "            print('Error: Betting line outside bounds [100,+inf]')\n",
    "        else:\n",
    "            pr = num/(100+num)\n",
    "            return pr\n",
    "\n",
    "    elif x.find('+') > -1:\n",
    "        num = int(x.replace(' ','').replace('+',''))\n",
    "        if num < 100 or num > 999:\n",
    "            print('Error: Betting line outside bounds [100,999]')\n",
    "        else:\n",
    "            pr = 100/(100+num)\n",
    "            return pr\n",
    "\n",
    "    else:\n",
    "        print('Error: No sign found in betting line string')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = pd.DataFrame([['-120','-120+'],['+214','214'],['-123','-1123'],['-120','+90']])\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for y in df.columns:\n",
    "    df[y] = df[y].apply(lambda x : american_to_probability(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set chrome options"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all column names from `skr_statfox_matchups_cols.py` run"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# all_cols = pd.read_pickle('pickles/statfox_matchups_cols.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get list of games"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "glhead = pd.read_csv('GLHEADER.CSV',header=None)\n",
    "gms = pd.read_csv('GL2018.CSV',header=0,names=list(glhead[0]))\n",
    "gms = gms[['date','team_h','team_v','score_h','score_v']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### translate 3 letter team name to full"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "teams = pd.read_csv('TEAM_NAMES.CSV',header=0,index_col=['name1'],usecols=['name1','name3'])\n",
    "teams = teams['name3'].to_dict()\n",
    "\n",
    "gms['team_h'] = gms['team_h'].map(lambda x: teams[x.upper()])\n",
    "gms['team_v'] = gms['team_v'].map(lambda x: teams[x.upper()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert db connection string. Eff security or other good practices for now"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "engine = create_engine('mysql+mysqlconnector://jsimssy:1u2kIr&vDr8!lal@ml-pipeline-01.cf1vgzngw30x.us-east-2.rds.amazonaws.com:3306/mlb_bet_dev?use_pure=True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main scrape code"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "dt = ''\n",
    "tm_h = ''\n",
    "tm_v = ''\n",
    "sc_h = 0\n",
    "sc_v = 0\n",
    "\n",
    "# for x in range(len(gms)):\n",
    "# for x in range(43,47):\n",
    "for x in range(52,55):\n",
    "    \n",
    "    try:\n",
    "        dt = str(gms.loc[x,'date'])\n",
    "        tm_h = str(gms.loc[x,'team_h']).replace(' ','')                                                              \n",
    "        tm_v = str(gms.loc[x,'team_v']).replace(' ','')\n",
    "        sc_h = gms.loc[x,'score_h']\n",
    "        sc_v = gms.loc[x,'score_v']\n",
    "        \n",
    "        # Adjust URL if second game of double header\n",
    "        if (x > 1) and (str(gms.loc[x-1,'date']) == dt) and (str(gms.loc[x-1,'team_h']).replace(' ','') == tm_h):\n",
    "            tm_h = tm_h+'2'\n",
    "            \n",
    "        url = 'http://foxsheets.statfoxsports.com/foxsheets.aspx?s=mlb&g='+dt+tm_h+'&r=at'\n",
    "        driver = webdriver.Chrome(options=options,executable_path='/home/ec2-user/./chromedriver')\t\n",
    "        driver.get(url)\t\n",
    "        ls = pd.DataFrame([])\n",
    "        ls_a = pd.DataFrame([])\n",
    "        \n",
    "        # Primary table in schema. Game, team, result, odds. Indexed on YYYYMMDDBBB where B is board number\n",
    "        sub_root = '//*[@id=\"dnn_ctr490_View_UP\"]/table[3]/tbody/tr[9]/td/table/tbody/tr/td/table/tbody/tr/td[2]/table/tbody/tr/td/table/tbody/tr/td'\n",
    "        stats = driver.find_element_by_xpath(sub_root)\n",
    "        content_html = stats.get_attribute(\"innerHTML\")\t\n",
    "#         brd_v = pd.read_html(content_html)[0][0].loc[2]\n",
    "#         brd_h = pd.read_html(content_html)[0][0].loc[3]\n",
    "        games = pd.DataFrame([[1,int(dt)],[1,int(dt)]], columns=['game_id','date'])\n",
    "        games[['board','team_name','line_open','line_close']] = pd.read_html(content_html)[0].iloc[2:4][[0,1,2,4]].reset_index(drop=True)\n",
    "        for c in ['line_open','line_close']:\n",
    "            games[c] = games[c].apply(lambda x : american_to_probability(x))\n",
    "        games['game_id'] = (games['date'])*1000 + pd.to_numeric(games['board'])\n",
    "        games = games.drop(columns=['board'])\n",
    "        games['date'] = pd.to_datetime(gms['date'][x], format=\"%Y%m%d\")\n",
    "        games['home'] = [False,True]\n",
    "        games.to_sql(name='games', con=engine, if_exists='append', index=False)        \n",
    "        \n",
    "        # Dont increase tr for regular (non double header) games\n",
    "        plus1 = 0\n",
    "        \n",
    "    # Account for double headers\n",
    "    except:\n",
    "        try:            \n",
    "            # Get Game Board values for each team to use as index with date\n",
    "            sub_root = '//*[@id=\"dnn_ctr490_View_UP\"]/table[3]/tbody/tr[10]/td/table/tbody/tr/td/table/tbody/tr/td[2]/table/tbody/tr/td/table/tbody/tr/td'\n",
    "            stats = driver.find_element_by_xpath(sub_root)\n",
    "            content_html = stats.get_attribute(\"innerHTML\")\t\n",
    "            brd_v = pd.read_html(content_html)[0][0].loc[2]\n",
    "            brd_h = pd.read_html(content_html)[0][0].loc[3]\n",
    "            \n",
    "            # Increase tr in main block below by 1\n",
    "            plus1 = 1\n",
    "            \n",
    "        except:\n",
    "            print('Error: Did not retreive game and board info for match')\n",
    "            print('...url was: ',url)\n",
    "            print('...loop iteration was: ',x)\n",
    "            continue\n",
    "    \n",
    "#     for tr in [14,15,16,18,19,20,23,25]:\n",
    "    for tr in [14]:\n",
    "        \n",
    "#         try:       \n",
    "            # Adjust tr if double header (tr + plus1)\n",
    "            sub_root = '//*[@id=\"dnn_ctr490_View_UP\"]/table[3]/tbody/tr['+str(tr+plus1)+']/td/table'\n",
    "            stats = driver.find_element_by_xpath(sub_root)\n",
    "            content_html = stats.get_attribute(\"innerHTML\")\t\n",
    "            \n",
    "            if tr in [14,18]:\n",
    "                if tr == 14:\n",
    "                    section = 'overall_v'\n",
    "                    board = brd_v\n",
    "                elif tr == 18:\n",
    "                    section = 'overall_h'\n",
    "                    board = brd_h\n",
    "                ls = pd.read_html(content_html,index_col=[0],header=[2])[0]             \n",
    "                # Parse columns with multiple values e.g. Win and Loss or Over and Under\n",
    "                ls[['W','L']] = ls['W-L'].str.split(\"-\",expand=True)\n",
    "                ls[['O','U']] = ls['O-U'].str.split(\"-\",expand=True)\n",
    "                ls = ls.drop(columns=['W-L','O-U'])                \n",
    "                # Make sql table for each row index (All games, night games, etc.)\n",
    "                for i in ls.index:\n",
    "                    Y = pd.DataFrame(ls.loc[i].apply(lambda x : float64_to_float(x))).transpose()\n",
    "                    Y['game_id'] = dt+'_'+board\n",
    "                    Y = Y.set_index(['game_id'],drop=True)\n",
    "                    table = section+'_'+i\n",
    "                    Y.to_sql(name=table, con=engine, if_exists='append', index=False)\n",
    "                \n",
    "            elif tr in [15,19]:\n",
    "                if tr == 15:\n",
    "                    section = 'hitNfield_v'\n",
    "                    board = brd_v\n",
    "                elif tr == 19:\n",
    "                    section = 'hitNfield_h'\n",
    "                    board = brd_h\n",
    "                ls = pd.read_html(content_html,index_col=[0],header=[2])[0]                                \n",
    "                # Make sql table for each row index (All games, night games, etc.)\n",
    "                for i in ls.index:\n",
    "                    Y = pd.DataFrame(ls.loc[i].apply(lambda x : float64_to_float(x))).transpose()\n",
    "                    Y['idx'] = dt+'_'+board\n",
    "                    table = section+'_'+i\n",
    "                    Y.to_sql(name=table, con=engine, if_exists='append', index=False)\n",
    "                \n",
    "            elif tr in [16,20]:\n",
    "                if tr == 16:\n",
    "                    section = 'bullpen_v'\n",
    "                    board = brd_v\n",
    "                elif tr == 20:\n",
    "                    section = 'bullpen_h'\n",
    "                    board = brd_h\n",
    "                ls = pd.read_html(content_html,index_col=[0],header=[1])[0]                                \n",
    "                ls[['W','L']] = ls['W-L'].str.split(\"-\",expand=True)\n",
    "                ls = ls.drop(columns=['W-L'])                \n",
    "                # Make sql table for each row index (All games, night games, etc.)\n",
    "                for i in ls.index:\n",
    "                    Y = pd.DataFrame(ls.loc[i].apply(lambda x : float64_to_float(x))).transpose()\n",
    "                    Y['idx'] = dt+'_'+board\n",
    "                    table = section+'_'+i\n",
    "                    Y.to_sql(name=table, con=engine, if_exists='append', index=False)\n",
    "                \n",
    "            elif tr in [23,25]:\n",
    "                if tr == 23:\n",
    "                    section = 'matchups_v'\n",
    "                    board = brd_v\n",
    "                elif tr == 25:\n",
    "                    section = 'matchups_h'\n",
    "                    board = brd_h\n",
    "                ls = pd.read_html(content_html,header=[2])[0]    \n",
    "                # Make pitching matchup date relative to game day, not raw dates\n",
    "                ls['Date'] = pd.to_datetime(ls['Date']) - pd.to_datetime(gms['date'][x], format=\"%Y%m%d\")\n",
    "                ls = ls.set_index('Date')\n",
    "                # Split Score and Tot, Ovr/Und to multiple variables\n",
    "                ls[['score_for','score_against']] = ls['Score'].str.split(\"-\",expand=True)\n",
    "                ls[['ovrund_num','ovrund']] = ls['Tot.'].str.split(\" \",expand=True)\n",
    "                ls = ls.drop(columns=['Score','Tot.'])      \n",
    "                \n",
    "                # Make sql table for each row index (All games, night games, etc.)\n",
    "                for i in ls.index:\n",
    "                    # Only want negative (past) matchup results, future games are null\n",
    "                    if i.days < 0:\n",
    "                        Y = pd.DataFrame(ls.loc[i].apply(lambda x : float64_to_float(x))).transpose()\n",
    "                        Y['idx'] = int(dt)*1000 + int(board)\n",
    "                        table = section+'_'+str(i.days)\n",
    "                        Y.to_sql(name=table, con=engine, if_exists='append', index=False)\n",
    "                \n",
    "#             ls = pd.concat([ls], keys=[table])\n",
    "#             ls = ls.stack()\n",
    "#             ls_a = pd.concat([ls_a,ls])       "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "        except:\n",
    "            # Check for normal pagenotfound verses error parsing\n",
    "            try:\n",
    "                xpath_notfound = '//*[@src=\"/images/foxsheetnotfound.gif\"]'\n",
    "                driver.find_element_by_xpath(xpath_notfound)\n",
    "                print('No game exists for ',tm_h,' on ',dt)\n",
    "                print('Error: loop iteration = ',x)\n",
    "                continue\n",
    "                \n",
    "            except:\n",
    "                err_msg = 'Possible change to HTML tables structure \\n'\n",
    "                err_msg += err_msg + 'Check the URL: ' + url\n",
    "                print('Error: loop iteration = ',x)\n",
    "                continue\n",
    "\n",
    "    # Close all Chrome sessions to save memory between loops\n",
    "    driver.quit()          "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "games[['line_open','line_close']].apply(american_to_probability, axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "games"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "                for i in ls.index:\n",
    "                    Y = pd.DataFrame(ls.loc[i].apply(lambda x : float64_to_float(x))).transpose()\n",
    "                    Y['game_id'] = dt+'_'+board\n",
    "                    Y = Y.set_index(['game_id'],drop=True)\n",
    "                    table = section+'_'+i\n",
    "                    Y.to_sql(name=table, con=engine, if_exists='append', index=True, index_labels=['game_id'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Y.to_sql?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "board = brd_v \n",
    "Y = pd.DataFrame([int(dt)*1000 + int(board)], columns=['idx'])\n",
    "Y = Y.concat(ls.loc[i].apply(lambda x : float64_to_float(x)).transpose())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "x = 45\n",
    "\n",
    "dt = str(gms.loc[x,'date'])\n",
    "tm_h = str(gms.loc[x,'team_h']).replace(' ','')                                                              \n",
    "tm_v = str(gms.loc[x,'team_v']).replace(' ','')\n",
    "sc_h = gms.loc[x,'score_h']\n",
    "sc_v = gms.loc[x,'score_v']\n",
    "\n",
    "# Adjust URL if second game of double header\n",
    "if (x > 1) and (str(gms.loc[x-1,'date']) == dt) and (str(gms.loc[x-1,'team_h']).replace(' ','') == tm_h):\n",
    "    tm_h = tm_h+'2'\n",
    "\n",
    "url = 'http://foxsheets.statfoxsports.com/foxsheets.aspx?s=mlb&g='+dt+tm_h+'&r=at'\n",
    "driver = webdriver.Chrome(options=options,executable_path='/home/ec2-user/./chromedriver')\t\n",
    "driver.get(url)\t\n",
    "ls = pd.DataFrame([])\n",
    "ls_a = pd.DataFrame([])\n",
    "\n",
    "# Get Game Board values for each team to use as index with date\n",
    "sub_root = '//*[@id=\"dnn_ctr490_View_UP\"]/table[3]/tbody/tr[9]/td/table/tbody/tr/td/table/tbody/tr/td[2]/table/tbody/tr/td/table/tbody/tr/td'\n",
    "stats = driver.find_element_by_xpath(sub_root)\n",
    "content_html = stats.get_attribute(\"innerHTML\")\t\n",
    "brd_v = pd.read_html(content_html)[0][0].loc[2]\n",
    "brd_h = pd.read_html(content_html)[0][0].loc[3]\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Y.to_sql(name=table, con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "pd.read_html(content_html)[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "pd.read_html(content_html,index_col=[0],header=[2])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X = pd.read_html(content_html,index_col=[0],header=[2])[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.Series(X.loc['All Games'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.DataFrame([],index=[[int(dt)],[int(brd_v)]])\n",
    "Y = pd.DataFrame([])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for x in X.index:\n",
    "    Y = pd.DataFrame(X.loc[x].apply(lambda x : float64_to_float(x))).transpose()\n",
    "    Y['idx'] = dt+'_'+board\n",
    "    Y.to_sql(name=table+x, con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "z = Y.set_index(keys=list(['game_date'],['board']),drop=True).index\n",
    "z"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.DataFrame?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Y.set_index(keys=['game_date','board'],drop=True).index.levels"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Y.to_sql?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    ls_a = ls_a.transpose()\n",
    "\n",
    "    ls_a['score_v'] = sc_v\n",
    "    ls_a['team_v'] = tm_v\n",
    "    ls_a['team_h'] = tm_h\n",
    "    ls_a['score_h'] = sc_h  \n",
    "    \n",
    "    all_cols = all_cols.append(ls_a, sort=False).reset_index(drop=True).tail(1)\n",
    "    \n",
    "    all_cols = all_cols.transpose().reset_index(drop=True).transpose()\n",
    "    \n",
    "    all_cols.to_sql(name='statfox_matchups', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_cols.to_sql?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_cols = all_cols.append(ls_a, sort=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X.to_sql(name='statfox_matchups', con=engine, if_exists='update', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X = pd.DataFrame({'a':[1,3,6]})\n",
    "Y = pd.DataFrame({'b':[4,2,5]})\n",
    "Y.append(X)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pk.reset_index(drop=True).transpose()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pk.transpose().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pk.reset_index?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    # Save each row to CSV. Maybe better to use pickles get all possible fields first to make final SQL table\n",
    "    if os.path.isfile(fname) == False:\n",
    "        ls_a.to_csv(fname,index=False)\n",
    "    else:\n",
    "        with open(fname, 'a') as f1:\n",
    "            ls_a.to_csv(f1, index=False, header=False)        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ls = pd.read_html(content_html,header=[2])[0]    \n",
    "ls"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ls['Date'] = pd.to_datetime(ls['Date']) - pd.to_datetime(gms['date'][x], format=\"%Y%m%d\")\n",
    "ls"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ls[['score_for','score_against']] = ls['Score'].str.split(\"-\",expand=True)\n",
    "ls[['ovrund_num','ovrund']] = ls['Tot.'].str.split(\" \",expand=True)\n",
    "ls = ls.drop(columns=['Score','Tot.'])      \n",
    "ls = pd.concat([ls], keys=[table])\n",
    "\n",
    "ls = ls.stack()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mdx = ls.index\n",
    "mdx = mdx.set_levels([mdx.levels[0], tdelta, mdx.levels[2]])\n",
    "mdx"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "s = ls\n",
    "s.reset_index(level=[1],drop=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.Series(data=ls.values).reset_index"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.Series(data=ls.values,index=mdx"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mdx.codes[2]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mdx"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Xls = pd.Series"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.to_datetime(gms['date'][x], format=\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "idx = pd.MultiIndex.from_tuples([(1, u'one'), (1, u'two'),\n",
    "                                    (2, u'one'), (2, u'two')],\n",
    "                                    names=['foo', 'bar'])\n",
    "idx"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "arrays = [[1, 1, 2, 2], [6,7], ['red', 'blue', 'red', 'blue']]\n",
    "mdx = pd.MultiIndex.from_product(arrays, names=('number', 'middle', 'color'))\n",
    "mdx = mdx.set_levels([mdx.levels[0],['a','b'],mdx.levels[2]])\n",
    "mdx"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.MultiIndex.from_product?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Xls.index.levels[1] = pd.to_datetime(Xls.index.levels[1]) - pd.to_datetime(gms['date'][x], format=\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Xls2 = Xls.index.set_levels([pd.to_datetime(Xls.index.levels[1]) - pd.to_datetime(gms['date'][x], format=\"%Y%m%d\")])\n",
    "Xls2 = Xls.index.swaplevel(i=2,j=1)\n",
    "Xls2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Xls3 = Xls.reindex(Xls2)\n",
    "Xls3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Xls2 = Xls.index.swaplevel"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[pd.to_datetime(ls.index.levels[1]) - pd.to_datetime(gms['date'][x], format=\"%Y%m%d\")]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Xls.index.set_levels([pd.to_datetime(ls.index.levels[1]) - pd.to_datetime(gms['date'][x], format=\"%Y%m%d\")])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Xls.reindex(pd.to_datetime(ls.index.levels[1]) - pd.to_datetime(gms['date'][x], format=\"%Y%m%d\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ls.index"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.to_pickle(ls,'pickles/matchups_h.zip')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ls = []\n",
    "ls = pd.read_pickle('pickles/matchups_h.zip')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ar = df.team_v.unique()\n",
    "ar = np.sort(ar,axis=0)\n",
    "df = pd.DataFrame([ar]).transpose()\n",
    "df = df.rename(index=int, columns={0: \"team\"})\n",
    "\n",
    "gl = pd.read_csv('GL2018.CSV', header=0, index_col=0, nrows=test_rows,\n",
    "                     usecols=['date','team_h','team_v','score_h', 'score_v'],\n",
    "                     engine='c',\n",
    "                     parse_dates=['date'],\n",
    "                     infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cols = list(range(len(ls_a.transpose())))\n",
    "for n in range(len(ls_a.transpose())):\n",
    "    cols[n] = 'feat_'+str(n)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "soup = BeautifulSoup(element_html, \"html.parser\")\n",
    "for td in tds:\n",
    "    soup_all = soup.find_all('td',{'class': 'matchupCells Text'})\n",
    "    soup_all[td].content\n",
    "    vals = pd.concat([vals,val])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.read_html(soup_all)[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.read_html?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "index.to_flat_index()\n",
    "\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lsall = pd.concat([ls1.melt().dropna(),ls2.melt().dropna(),ls3.melt().dropna(),ls4.melt().dropna()])\n",
    "lsall"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "xt = 3\n",
    "if xt in [5,2]:\n",
    "    print('One')\n",
    "elif xt in [3,4]:\n",
    "    print('Two')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df2 = df.loc[3:10,1:10]\n",
    "df2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df3 = df2.reindex(index=list(df.loc[3:10,0]),columns=list(df.loc[2,1:10]))\n",
    "df3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df2 = pd.DataFrame(df.loc[3:10,1:10],index=list(df.loc[3:10,0]),columns=list(df.loc[2,1:10]))\n",
    "df2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "arr = [df.loc[3:10,1:10]]\n",
    "arr?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = lsall.iloc[2:10,0:10]\n",
    "df2 = df.melt(id_vars=[0],value_vars=range(1,10))\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.iloc[2,1:10].to_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Xdf = lsall.reindex(range(len(lsall)))\n",
    "Xdf"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X = pd.DataFrame(df[1], index=\n",
    "\n",
    "pd.DataFrame?\n",
    "\n",
    "soup = BeautifulSoup(content_html, \"html.parser\")\n",
    "\n",
    "stats = soup.find('table',{'class': 'bg2'}).contents[0]\n",
    "\n",
    "stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
