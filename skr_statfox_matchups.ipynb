{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape StatFox for Current and Historical Matchup Stats\n",
    "`Notebooks/skr_statfox.ipynb`\n",
    "###### `BY: Jonathan Sims` \n",
    "###### `CREATED: 2019-06-15`\n",
    "- GOAL: Scrape daily to build up historical matchup tables\n",
    "- USE: Treat each matchup page as one obs in RF model to asses feature importance\n",
    "    - Useful before spending time building up game/pitch/player-level count data \n",
    "    - Compare odds here with odds from VegasInsider to look for odds movement features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FromPickleS3(bucketname, keyname):\n",
    "    \"\"\"Get dataframe from s3 and unpickle\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    pkldf = s3.get_object(Bucket=bucketname,Key=keyname)['Body'].read()\n",
    "    df = pickle.loads(pkldf)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Flatten2DLabel(obj, row_label, col_label):\n",
    "    \"\"\"Flattens a dataframe to single row and concatenates row and \n",
    "    column lables within the dataframe into one column name.                                 \n",
    "    \"\"\"\n",
    "    # Return all row indexes after last row_label\n",
    "    st_row = row_label[-1]+1\n",
    "    # Return all columns after last col_label\n",
    "    st_col = col_label[-1]+1\n",
    "\n",
    "\n",
    "    # Get row labels and concatenate\n",
    "    for x in row_label:\n",
    "        if x == row_label[0]:\n",
    "            cols = []\n",
    "            cols = obj.iloc[x][st_col:]\n",
    "        else:\n",
    "            cols = cols+'_'+obj.iloc[x][st_col:]\n",
    "\n",
    "    # Get column labels and concatenate\n",
    "    for x  in col_label:\n",
    "        if x == col_label[0]:\n",
    "            rows = []\n",
    "            rows = obj[st_row:][x]\n",
    "        else:\n",
    "            rows = rows+'_'+obj[st_row:][x]  \n",
    "\n",
    "    # Strip non alphanumeric\n",
    "    colstrip = cols.str.replace('[^\\w]','')\n",
    "    rowstrip = rows.str.replace('[^\\w]','')\n",
    "\n",
    "\n",
    "    obj_in = pd.DataFrame([])\n",
    "    for x in rowstrip.index:\n",
    "        for y in colstrip.index:\n",
    "            obj_tmp = pd.DataFrame([obj.iloc[x][y]], columns=[str(colstrip[y])+'_'+str(rowstrip[x])])\n",
    "            obj_in = pd.concat([obj_in,obj_tmp], axis=1)\n",
    "            \n",
    "    return obj_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanStatfoxScrape(obj):\n",
    "    \"\"\"Takes flattened sub-tables from statfox matchup\n",
    "    page and cleans and shortens feature names\n",
    "    \"\"\"\n",
    "    # Team names to V or H\n",
    "    obj.columns = obj.columns.str.replace(tm_v,'V_')\n",
    "    obj.columns = obj.columns.str.replace(tm_h,'H_')\n",
    "\n",
    "    # Shorten sub-table names\n",
    "    obj.columns = obj.columns.str.replace('CurrentSeasonPerformance', 'Overall')\n",
    "    obj.columns = obj.columns.str.replace('TeamHittingandFieldingStatistics','HitField')\n",
    "    obj.columns = obj.columns.str.replace('BullpenPitchingStatistics','Bullpen')\n",
    "    \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DuplicatedVarnames(df):\n",
    "    \"\"\"Return a dict of all variable names that \n",
    "    are duplicated in a given dataframe.\"\"\"\n",
    "    repeat_dict = {}\n",
    "    var_list = list(df) # list of varnames as strings\n",
    "    for varname in var_list:\n",
    "        # make a list of all instances of that varname\n",
    "        test_list = [v for v in var_list if v == varname] \n",
    "        # if more than one instance, report duplications in repeat_dict\n",
    "        if len(test_list) > 1: \n",
    "            repeat_dict[varname] = len(test_list)\n",
    "    return repeat_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get list of games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "glhead = pd.read_csv('GLHEADER.CSV',header=None)\n",
    "gms = pd.read_csv('GL2018.CSV',header=0,names=list(glhead[0]))\n",
    "gms = gms[['date','team_h','team_v','score_h','score_v']]\n",
    "\n",
    "teams = pd.read_csv('TEAM_NAMES.CSV',header=0,index_col=['name1'],usecols=['name1','name3'])\n",
    "teams = teams['name3'].to_dict()\n",
    "\n",
    "gms['team_h'] = gms['team_h'].map(lambda x: teams[x.upper()])\n",
    "gms['team_v'] = gms['team_v'].map(lambda x: teams[x.upper()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get dataframe from s3 and unpickle\n",
    "#### Format and append each matchup together from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On match:  20180329 ATLANTA\n"
     ]
    }
   ],
   "source": [
    "df_all = pd.DataFrame()\n",
    "\n",
    "for x in gms.index:\n",
    "# for x in range(10):\n",
    "    # Parse date, team names, and score from games list\n",
    "    dt = str(gms.loc[x,'date'])\n",
    "    tm_h = str(gms.loc[x,'team_h']).replace(' ','')                                                              \n",
    "    tm_v = str(gms.loc[x,'team_v']).replace(' ','')\n",
    "    sc_h = gms.loc[x,'score_h']\n",
    "    sc_v = gms.loc[x,'score_v']\n",
    "\n",
    "    # Adjust URL if second game of double header\n",
    "    if (x > 1) and (str(gms.loc[x-1,'date']) == dt) and (str(gms.loc[x-1,'team_h']).replace(' ','') == tm_h):\n",
    "        tm_h_url = tm_h+'2'\n",
    "    else:\n",
    "        tm_h_url = tm_h\n",
    "        \n",
    "    # Create dataframe from s3 pickle\n",
    "    df = FromPickleS3(bucketname='scrapes-rawhtml-dev', keyname='statfox_DEV/'+dt+tm_h_url+'.pkl')\n",
    "\n",
    "    # Show progress\n",
    "    if x%100 == 0:\n",
    "        print('On match: ', dt, tm_h_url)\n",
    "\n",
    "#### Parse and concatenate all features together, by matchup\n",
    "\n",
    "    # 6:  Overall - board and line\n",
    "    # 11: Away - Current Season Performance\n",
    "    # 12: Away - Team Hitting and Fielding\n",
    "    # 13: Away - Bullpen Pitching \n",
    "    # 14: Home - Current Season Performance\n",
    "    # 15: Home - Team Hitting and Fielding\n",
    "    # 16: Home - Bullpen Pitching \n",
    "    df6  = df[6] \n",
    "    df11 = df[11]\n",
    "    df12 = df[12]\n",
    "    df13 = df[13]\n",
    "    df14 = df[14]\n",
    "    df15 = df[15]\n",
    "    df16 = df[16]\n",
    "\n",
    "    df6 = df6.transpose() # Make board and team labels first\n",
    "    df6_wd = Flatten2DLabel(df6, [1], [0,1])\n",
    "\n",
    "    df11_wd = Flatten2DLabel(df11, [0,1,2], [0])\n",
    "    df14_wd = Flatten2DLabel(df14, [0,1,2], [0])\n",
    "\n",
    "    df12_wd = Flatten2DLabel(df12, [0,1,2], [0])\n",
    "    df15_wd = Flatten2DLabel(df15, [0,1,2], [0])\n",
    "\n",
    "    df13_wd = Flatten2DLabel(df13, [0,1], [0])\n",
    "    df16_wd = Flatten2DLabel(df16, [0,1], [0])\n",
    "\n",
    "#### Concatenate all sub-tables and clean labels\n",
    "\n",
    "    df_wd_all = pd.concat([df6_wd, df11_wd, df12_wd, df13_wd, df14_wd, df15_wd, df16_wd], axis=1)\n",
    "    df_wd_all = CleanStatfoxScrape(df_wd_all)\n",
    "\n",
    "#### Add matchup index value date-board_h-board_v\n",
    "\n",
    "    board_h = df6[3].iloc[0]\n",
    "    board_v = df6[2].iloc[0]\n",
    "    df_wd_all.index = [int(dt)*1000000+int(board_h)*1000+int(board_v)]\n",
    "    \n",
    "#### Add team names and final scores\n",
    "\n",
    "    df_wd_all['tm_h'] = tm_h\n",
    "    df_wd_all['tm_v'] = tm_v\n",
    "    df_wd_all['sc_h'] = sc_h\n",
    "    df_wd_all['sc_v'] = sc_v\n",
    "    \n",
    "#### Drop columns duplicates (usually nan)\n",
    "\n",
    "    for x in DuplicatedVarnames(df_wd_all):\n",
    "        df_wd_all = df_wd_all.drop(list(df_wd_all.filter(regex=x)), axis=1)\n",
    "        \n",
    "#### Split features on hyphen if OU (over-under) or WL (win-loss)        \n",
    "    \n",
    "    ou_cols = [col for col in df_wd_all.columns if 'OU' in col]\n",
    "    \n",
    "    if ou_cols != []:\n",
    "        for x in ou_cols:\n",
    "            xO = x.replace('OU', 'O')\n",
    "            xU = x.replace('OU', 'U')\n",
    "            df_wd_all[[xO,xU]] = df_wd_all[x].str.split(\"-\",expand=True)    \n",
    "            df_wd_all = df_wd_all.drop(x, axis=1)\n",
    "\n",
    "    wl_cols = [col for col in df_wd_all.columns if 'WL' in col]\n",
    "    \n",
    "    if wl_cols != []:\n",
    "        for x in wl_cols:\n",
    "            xW = x.replace('WL', 'W')\n",
    "            xL = x.replace('WL', 'L')\n",
    "            df_wd_all[[xW,xL]] = df_wd_all[x].str.split(\"-\",expand=True)    \n",
    "            df_wd_all = df_wd_all.drop(x, axis=1)    \n",
    "\n",
    "#### Create target variable (home win)\n",
    "\n",
    "    df_wd_all['win_h'] = np.where(df_wd_all['sc_h'] > df_wd_all['sc_v'], 1, 0)\n",
    "    \n",
    "#### Combine feature rows\n",
    "\n",
    "    df_all = pd.concat([df_all, df_wd_all], ignore_index=False, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save dataframe to tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv('20200222.skr_statfox.2018.tsv.gz', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset target variable and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-a00fa6dcbb96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "for x in gms.index:\n",
    "    print(x.type)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_tar = df_all['win_h']\n",
    "df_ft = df_all.drop(['V__Latest_Line', 'V__Latest_Total', \n",
    "                     'V__Opening_Line', 'V__Opening_Total', \n",
    "                     'sc_h', 'sc_v', 'win_h'], \n",
    "                    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `20200222`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MySQL Connector needs Python float, not numpy float64"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "def float64_to_float(x):\n",
    "    try:\n",
    "        if x.dtype == 'float64':\n",
    "            return float(x)\n",
    "    except:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert American betting lines to probability value satisfying E[x]=0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "def american_to_probability(x):\n",
    "    \"\"\"\n",
    "    Turns American +/- odds into probability 0 to 1 inclusive\n",
    "    \"\"\"\n",
    "    if x.find('-') > -1 and x.find('+') > -1:\n",
    "        print('Error: + and - signs found in betting line string') \n",
    "\n",
    "    elif x.find('-') > -1:\n",
    "        num = int(x.replace(' ','').replace('-',''))\n",
    "        if num < 100:\n",
    "            print('Error: Betting line outside bounds [100,+inf]')\n",
    "        else:\n",
    "            pr = num/(100+num)\n",
    "            return pr\n",
    "\n",
    "    elif x.find('+') > -1:\n",
    "        num = int(x.replace(' ','').replace('+',''))\n",
    "        if num < 100 or num > 999:\n",
    "            print('Error: Betting line outside bounds [100,999]')\n",
    "        else:\n",
    "            pr = 100/(100+num)\n",
    "            return pr\n",
    "\n",
    "    else:\n",
    "        print('Error: No sign found in betting line string')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = pd.DataFrame([['-120','-120+'],['+214','214'],['-123','-1123'],['-120','+90']])\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for y in df.columns:\n",
    "    df[y] = df[y].apply(lambda x : american_to_probability(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set chrome options"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all column names from `skr_statfox_matchups_cols.py` run"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# all_cols = pd.read_pickle('pickles/statfox_matchups_cols.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get list of games"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "glhead = pd.read_csv('GLHEADER.CSV',header=None)\n",
    "gms = pd.read_csv('GL2018.CSV',header=0,names=list(glhead[0]))\n",
    "gms = gms[['date','team_h','team_v','score_h','score_v']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### translate 3 letter team name to full"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "teams = pd.read_csv('TEAM_NAMES.CSV',header=0,index_col=['name1'],usecols=['name1','name3'])\n",
    "teams = teams['name3'].to_dict()\n",
    "\n",
    "gms['team_h'] = gms['team_h'].map(lambda x: teams[x.upper()])\n",
    "gms['team_v'] = gms['team_v'].map(lambda x: teams[x.upper()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert db connection string. Eff security or other good practices for now"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "engine = create_engine('mysql+mysqlconnector://jsimssy:1u2kIr&vDr8!lal@ml-pipeline-01.cf1vgzngw30x.us-east-2.rds.amazonaws.com:3306/mlb_bet_dev?use_pure=True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main scrape code"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "dt = ''\n",
    "tm_h = ''\n",
    "tm_v = ''\n",
    "sc_h = 0\n",
    "sc_v = 0\n",
    "\n",
    "# for x in range(len(gms)):\n",
    "# for x in range(43,47):\n",
    "for x in range(52,55):\n",
    "    \n",
    "    try:\n",
    "        dt = str(gms.loc[x,'date'])\n",
    "        tm_h = str(gms.loc[x,'team_h']).replace(' ','')                                                              \n",
    "        tm_v = str(gms.loc[x,'team_v']).replace(' ','')\n",
    "        sc_h = gms.loc[x,'score_h']\n",
    "        sc_v = gms.loc[x,'score_v']\n",
    "        \n",
    "        # Adjust URL if second game of double header\n",
    "        if (x > 1) and (str(gms.loc[x-1,'date']) == dt) and (str(gms.loc[x-1,'team_h']).replace(' ','') == tm_h):\n",
    "            tm_h = tm_h+'2'\n",
    "            \n",
    "        url = 'http://foxsheets.statfoxsports.com/foxsheets.aspx?s=mlb&g='+dt+tm_h+'&r=at'\n",
    "        driver = webdriver.Chrome(options=options,executable_path='/home/ec2-user/./chromedriver')\t\n",
    "        driver.get(url)\t\n",
    "        ls = pd.DataFrame([])\n",
    "        ls_a = pd.DataFrame([])\n",
    "        \n",
    "        # Primary table in schema. Game, team, result, odds. Indexed on YYYYMMDDBBB where B is board number\n",
    "        sub_root = '//*[@id=\"dnn_ctr490_View_UP\"]/table[3]/tbody/tr[9]/td/table/tbody/tr/td/table/tbody/tr/td[2]/table/tbody/tr/td/table/tbody/tr/td'\n",
    "        stats = driver.find_element_by_xpath(sub_root)\n",
    "        content_html = stats.get_attribute(\"innerHTML\")\t\n",
    "#         brd_v = pd.read_html(content_html)[0][0].loc[2]\n",
    "#         brd_h = pd.read_html(content_html)[0][0].loc[3]\n",
    "        games = pd.DataFrame([[1,int(dt)],[1,int(dt)]], columns=['game_id','date'])\n",
    "        games[['board','team_name','line_open','line_close']] = pd.read_html(content_html)[0].iloc[2:4][[0,1,2,4]].reset_index(drop=True)\n",
    "        for c in ['line_open','line_close']:\n",
    "            games[c] = games[c].apply(lambda x : american_to_probability(x))\n",
    "        games['game_id'] = (games['date'])*1000 + pd.to_numeric(games['board'])\n",
    "        games = games.drop(columns=['board'])\n",
    "        games['date'] = pd.to_datetime(gms['date'][x], format=\"%Y%m%d\")\n",
    "        games['home'] = [False,True]\n",
    "        games.to_sql(name='games', con=engine, if_exists='append', index=False)        \n",
    "        \n",
    "        # Dont increase tr for regular (non double header) games\n",
    "        plus1 = 0\n",
    "        \n",
    "    # Account for double headers\n",
    "    except:\n",
    "        try:            \n",
    "            # Get Game Board values for each team to use as index with date\n",
    "            sub_root = '//*[@id=\"dnn_ctr490_View_UP\"]/table[3]/tbody/tr[10]/td/table/tbody/tr/td/table/tbody/tr/td[2]/table/tbody/tr/td/table/tbody/tr/td'\n",
    "            stats = driver.find_element_by_xpath(sub_root)\n",
    "            content_html = stats.get_attribute(\"innerHTML\")\t\n",
    "            brd_v = pd.read_html(content_html)[0][0].loc[2]\n",
    "            brd_h = pd.read_html(content_html)[0][0].loc[3]\n",
    "            \n",
    "            # Increase tr in main block below by 1\n",
    "            plus1 = 1\n",
    "            \n",
    "        except:\n",
    "            print('Error: Did not retreive game and board info for match')\n",
    "            print('...url was: ',url)\n",
    "            print('...loop iteration was: ',x)\n",
    "            continue\n",
    "    \n",
    "#     for tr in [14,15,16,18,19,20,23,25]:\n",
    "    for tr in [14]:\n",
    "        \n",
    "#         try:       \n",
    "            # Adjust tr if double header (tr + plus1)\n",
    "            sub_root = '//*[@id=\"dnn_ctr490_View_UP\"]/table[3]/tbody/tr['+str(tr+plus1)+']/td/table'\n",
    "            stats = driver.find_element_by_xpath(sub_root)\n",
    "            content_html = stats.get_attribute(\"innerHTML\")\t\n",
    "            \n",
    "            if tr in [14,18]:\n",
    "                if tr == 14:\n",
    "                    section = 'overall_v'\n",
    "                    board = brd_v\n",
    "                elif tr == 18:\n",
    "                    section = 'overall_h'\n",
    "                    board = brd_h\n",
    "                ls = pd.read_html(content_html,index_col=[0],header=[2])[0]             \n",
    "                # Parse columns with multiple values e.g. Win and Loss or Over and Under\n",
    "                ls[['W','L']] = ls['W-L'].str.split(\"-\",expand=True)\n",
    "                ls[['O','U']] = ls['O-U'].str.split(\"-\",expand=True)\n",
    "                ls = ls.drop(columns=['W-L','O-U'])                \n",
    "                # Make sql table for each row index (All games, night games, etc.)\n",
    "                for i in ls.index:\n",
    "                    Y = pd.DataFrame(ls.loc[i].apply(lambda x : float64_to_float(x))).transpose()\n",
    "                    Y['game_id'] = dt+'_'+board\n",
    "                    Y = Y.set_index(['game_id'],drop=True)\n",
    "                    table = section+'_'+i\n",
    "                    Y.to_sql(name=table, con=engine, if_exists='append', index=False)\n",
    "                \n",
    "            elif tr in [15,19]:\n",
    "                if tr == 15:\n",
    "                    section = 'hitNfield_v'\n",
    "                    board = brd_v\n",
    "                elif tr == 19:\n",
    "                    section = 'hitNfield_h'\n",
    "                    board = brd_h\n",
    "                ls = pd.read_html(content_html,index_col=[0],header=[2])[0]                                \n",
    "                # Make sql table for each row index (All games, night games, etc.)\n",
    "                for i in ls.index:\n",
    "                    Y = pd.DataFrame(ls.loc[i].apply(lambda x : float64_to_float(x))).transpose()\n",
    "                    Y['idx'] = dt+'_'+board\n",
    "                    table = section+'_'+i\n",
    "                    Y.to_sql(name=table, con=engine, if_exists='append', index=False)\n",
    "                \n",
    "            elif tr in [16,20]:\n",
    "                if tr == 16:\n",
    "                    section = 'bullpen_v'\n",
    "                    board = brd_v\n",
    "                elif tr == 20:\n",
    "                    section = 'bullpen_h'\n",
    "                    board = brd_h\n",
    "                ls = pd.read_html(content_html,index_col=[0],header=[1])[0]                                \n",
    "                ls[['W','L']] = ls['W-L'].str.split(\"-\",expand=True)\n",
    "                ls = ls.drop(columns=['W-L'])                \n",
    "                # Make sql table for each row index (All games, night games, etc.)\n",
    "                for i in ls.index:\n",
    "                    Y = pd.DataFrame(ls.loc[i].apply(lambda x : float64_to_float(x))).transpose()\n",
    "                    Y['idx'] = dt+'_'+board\n",
    "                    table = section+'_'+i\n",
    "                    Y.to_sql(name=table, con=engine, if_exists='append', index=False)\n",
    "                \n",
    "            elif tr in [23,25]:\n",
    "                if tr == 23:\n",
    "                    section = 'matchups_v'\n",
    "                    board = brd_v\n",
    "                elif tr == 25:\n",
    "                    section = 'matchups_h'\n",
    "                    board = brd_h\n",
    "                ls = pd.read_html(content_html,header=[2])[0]    \n",
    "                # Make pitching matchup date relative to game day, not raw dates\n",
    "                ls['Date'] = pd.to_datetime(ls['Date']) - pd.to_datetime(gms['date'][x], format=\"%Y%m%d\")\n",
    "                ls = ls.set_index('Date')\n",
    "                # Split Score and Tot, Ovr/Und to multiple variables\n",
    "                ls[['score_for','score_against']] = ls['Score'].str.split(\"-\",expand=True)\n",
    "                ls[['ovrund_num','ovrund']] = ls['Tot.'].str.split(\" \",expand=True)\n",
    "                ls = ls.drop(columns=['Score','Tot.'])      \n",
    "                \n",
    "                # Make sql table for each row index (All games, night games, etc.)\n",
    "                for i in ls.index:\n",
    "                    # Only want negative (past) matchup results, future games are null\n",
    "                    if i.days < 0:\n",
    "                        Y = pd.DataFrame(ls.loc[i].apply(lambda x : float64_to_float(x))).transpose()\n",
    "                        Y['idx'] = int(dt)*1000 + int(board)\n",
    "                        table = section+'_'+str(i.days)\n",
    "                        Y.to_sql(name=table, con=engine, if_exists='append', index=False)\n",
    "                \n",
    "#             ls = pd.concat([ls], keys=[table])\n",
    "#             ls = ls.stack()\n",
    "#             ls_a = pd.concat([ls_a,ls])       "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "        except:\n",
    "            # Check for normal pagenotfound verses error parsing\n",
    "            try:\n",
    "                xpath_notfound = '//*[@src=\"/images/foxsheetnotfound.gif\"]'\n",
    "                driver.find_element_by_xpath(xpath_notfound)\n",
    "                print('No game exists for ',tm_h,' on ',dt)\n",
    "                print('Error: loop iteration = ',x)\n",
    "                continue\n",
    "                \n",
    "            except:\n",
    "                err_msg = 'Possible change to HTML tables structure \\n'\n",
    "                err_msg += err_msg + 'Check the URL: ' + url\n",
    "                print('Error: loop iteration = ',x)\n",
    "                continue\n",
    "\n",
    "    # Close all Chrome sessions to save memory between loops\n",
    "    driver.quit()          "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "games[['line_open','line_close']].apply(american_to_probability, axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "games"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "                for i in ls.index:\n",
    "                    Y = pd.DataFrame(ls.loc[i].apply(lambda x : float64_to_float(x))).transpose()\n",
    "                    Y['game_id'] = dt+'_'+board\n",
    "                    Y = Y.set_index(['game_id'],drop=True)\n",
    "                    table = section+'_'+i\n",
    "                    Y.to_sql(name=table, con=engine, if_exists='append', index=True, index_labels=['game_id'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Y.to_sql?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "board = brd_v \n",
    "Y = pd.DataFrame([int(dt)*1000 + int(board)], columns=['idx'])\n",
    "Y = Y.concat(ls.loc[i].apply(lambda x : float64_to_float(x)).transpose())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "x = 45\n",
    "\n",
    "dt = str(gms.loc[x,'date'])\n",
    "tm_h = str(gms.loc[x,'team_h']).replace(' ','')                                                              \n",
    "tm_v = str(gms.loc[x,'team_v']).replace(' ','')\n",
    "sc_h = gms.loc[x,'score_h']\n",
    "sc_v = gms.loc[x,'score_v']\n",
    "\n",
    "# Adjust URL if second game of double header\n",
    "if (x > 1) and (str(gms.loc[x-1,'date']) == dt) and (str(gms.loc[x-1,'team_h']).replace(' ','') == tm_h):\n",
    "    tm_h = tm_h+'2'\n",
    "\n",
    "url = 'http://foxsheets.statfoxsports.com/foxsheets.aspx?s=mlb&g='+dt+tm_h+'&r=at'\n",
    "driver = webdriver.Chrome(options=options,executable_path='/home/ec2-user/./chromedriver')\t\n",
    "driver.get(url)\t\n",
    "ls = pd.DataFrame([])\n",
    "ls_a = pd.DataFrame([])\n",
    "\n",
    "# Get Game Board values for each team to use as index with date\n",
    "sub_root = '//*[@id=\"dnn_ctr490_View_UP\"]/table[3]/tbody/tr[9]/td/table/tbody/tr/td/table/tbody/tr/td[2]/table/tbody/tr/td/table/tbody/tr/td'\n",
    "stats = driver.find_element_by_xpath(sub_root)\n",
    "content_html = stats.get_attribute(\"innerHTML\")\t\n",
    "brd_v = pd.read_html(content_html)[0][0].loc[2]\n",
    "brd_h = pd.read_html(content_html)[0][0].loc[3]\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Y.to_sql(name=table, con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "pd.read_html(content_html)[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "pd.read_html(content_html,index_col=[0],header=[2])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X = pd.read_html(content_html,index_col=[0],header=[2])[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.Series(X.loc['All Games'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.DataFrame([],index=[[int(dt)],[int(brd_v)]])\n",
    "Y = pd.DataFrame([])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for x in X.index:\n",
    "    Y = pd.DataFrame(X.loc[x].apply(lambda x : float64_to_float(x))).transpose()\n",
    "    Y['idx'] = dt+'_'+board\n",
    "    Y.to_sql(name=table+x, con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "z = Y.set_index(keys=list(['game_date'],['board']),drop=True).index\n",
    "z"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.DataFrame?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Y.set_index(keys=['game_date','board'],drop=True).index.levels"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Y.to_sql?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    ls_a = ls_a.transpose()\n",
    "\n",
    "    ls_a['score_v'] = sc_v\n",
    "    ls_a['team_v'] = tm_v\n",
    "    ls_a['team_h'] = tm_h\n",
    "    ls_a['score_h'] = sc_h  \n",
    "    \n",
    "    all_cols = all_cols.append(ls_a, sort=False).reset_index(drop=True).tail(1)\n",
    "    \n",
    "    all_cols = all_cols.transpose().reset_index(drop=True).transpose()\n",
    "    \n",
    "    all_cols.to_sql(name='statfox_matchups', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_cols.to_sql?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_cols = all_cols.append(ls_a, sort=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X.to_sql(name='statfox_matchups', con=engine, if_exists='update', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X = pd.DataFrame({'a':[1,3,6]})\n",
    "Y = pd.DataFrame({'b':[4,2,5]})\n",
    "Y.append(X)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pk.reset_index(drop=True).transpose()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pk.transpose().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pk.reset_index?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    # Save each row to CSV. Maybe better to use pickles get all possible fields first to make final SQL table\n",
    "    if os.path.isfile(fname) == False:\n",
    "        ls_a.to_csv(fname,index=False)\n",
    "    else:\n",
    "        with open(fname, 'a') as f1:\n",
    "            ls_a.to_csv(f1, index=False, header=False)        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ls = pd.read_html(content_html,header=[2])[0]    \n",
    "ls"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ls['Date'] = pd.to_datetime(ls['Date']) - pd.to_datetime(gms['date'][x], format=\"%Y%m%d\")\n",
    "ls"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ls[['score_for','score_against']] = ls['Score'].str.split(\"-\",expand=True)\n",
    "ls[['ovrund_num','ovrund']] = ls['Tot.'].str.split(\" \",expand=True)\n",
    "ls = ls.drop(columns=['Score','Tot.'])      \n",
    "ls = pd.concat([ls], keys=[table])\n",
    "\n",
    "ls = ls.stack()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mdx = ls.index\n",
    "mdx = mdx.set_levels([mdx.levels[0], tdelta, mdx.levels[2]])\n",
    "mdx"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "s = ls\n",
    "s.reset_index(level=[1],drop=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.Series(data=ls.values).reset_index"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.Series(data=ls.values,index=mdx"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mdx.codes[2]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mdx"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Xls = pd.Series"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.to_datetime(gms['date'][x], format=\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "idx = pd.MultiIndex.from_tuples([(1, u'one'), (1, u'two'),\n",
    "                                    (2, u'one'), (2, u'two')],\n",
    "                                    names=['foo', 'bar'])\n",
    "idx"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "arrays = [[1, 1, 2, 2], [6,7], ['red', 'blue', 'red', 'blue']]\n",
    "mdx = pd.MultiIndex.from_product(arrays, names=('number', 'middle', 'color'))\n",
    "mdx = mdx.set_levels([mdx.levels[0],['a','b'],mdx.levels[2]])\n",
    "mdx"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.MultiIndex.from_product?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Xls.index.levels[1] = pd.to_datetime(Xls.index.levels[1]) - pd.to_datetime(gms['date'][x], format=\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Xls2 = Xls.index.set_levels([pd.to_datetime(Xls.index.levels[1]) - pd.to_datetime(gms['date'][x], format=\"%Y%m%d\")])\n",
    "Xls2 = Xls.index.swaplevel(i=2,j=1)\n",
    "Xls2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Xls3 = Xls.reindex(Xls2)\n",
    "Xls3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Xls2 = Xls.index.swaplevel"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[pd.to_datetime(ls.index.levels[1]) - pd.to_datetime(gms['date'][x], format=\"%Y%m%d\")]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Xls.index.set_levels([pd.to_datetime(ls.index.levels[1]) - pd.to_datetime(gms['date'][x], format=\"%Y%m%d\")])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Xls.reindex(pd.to_datetime(ls.index.levels[1]) - pd.to_datetime(gms['date'][x], format=\"%Y%m%d\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ls.index"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.to_pickle(ls,'pickles/matchups_h.zip')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ls = []\n",
    "ls = pd.read_pickle('pickles/matchups_h.zip')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ar = df.team_v.unique()\n",
    "ar = np.sort(ar,axis=0)\n",
    "df = pd.DataFrame([ar]).transpose()\n",
    "df = df.rename(index=int, columns={0: \"team\"})\n",
    "\n",
    "gl = pd.read_csv('GL2018.CSV', header=0, index_col=0, nrows=test_rows,\n",
    "                     usecols=['date','team_h','team_v','score_h', 'score_v'],\n",
    "                     engine='c',\n",
    "                     parse_dates=['date'],\n",
    "                     infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cols = list(range(len(ls_a.transpose())))\n",
    "for n in range(len(ls_a.transpose())):\n",
    "    cols[n] = 'feat_'+str(n)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "soup = BeautifulSoup(element_html, \"html.parser\")\n",
    "for td in tds:\n",
    "    soup_all = soup.find_all('td',{'class': 'matchupCells Text'})\n",
    "    soup_all[td].content\n",
    "    vals = pd.concat([vals,val])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.read_html(soup_all)[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.read_html?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "index.to_flat_index()\n",
    "\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lsall = pd.concat([ls1.melt().dropna(),ls2.melt().dropna(),ls3.melt().dropna(),ls4.melt().dropna()])\n",
    "lsall"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "xt = 3\n",
    "if xt in [5,2]:\n",
    "    print('One')\n",
    "elif xt in [3,4]:\n",
    "    print('Two')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df2 = df.loc[3:10,1:10]\n",
    "df2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df3 = df2.reindex(index=list(df.loc[3:10,0]),columns=list(df.loc[2,1:10]))\n",
    "df3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df2 = pd.DataFrame(df.loc[3:10,1:10],index=list(df.loc[3:10,0]),columns=list(df.loc[2,1:10]))\n",
    "df2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "arr = [df.loc[3:10,1:10]]\n",
    "arr?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = lsall.iloc[2:10,0:10]\n",
    "df2 = df.melt(id_vars=[0],value_vars=range(1,10))\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.iloc[2,1:10].to_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Xdf = lsall.reindex(range(len(lsall)))\n",
    "Xdf"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X = pd.DataFrame(df[1], index=\n",
    "\n",
    "pd.DataFrame?\n",
    "\n",
    "soup = BeautifulSoup(content_html, \"html.parser\")\n",
    "\n",
    "stats = soup.find('table',{'class': 'bg2'}).contents[0]\n",
    "\n",
    "stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
